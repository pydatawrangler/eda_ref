{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Getting Started with Polars\n",
    "format:\n",
    "  html:\n",
    "    self-contained: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.15.1\n",
    "  kernelspec:\n",
    "    display_name: 'Python [conda env:eda]'\n",
    "    language: python\n",
    "    name: conda-env-eda-py\n",
    "---\n",
    "\n",
    "# Getting Started with Polars\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Github repo for the Effective Polars Code is here:\n",
    "https://github.com/mattharrison/effective_polars_book\n",
    "\n",
    "Data Sets are on Github at this location:\n",
    "https://github.com/mattharrison/datasets/tree/master/data\n",
    "\n",
    "\n",
    "## Installing Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.21.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars Data Structures\n",
    "\n",
    "## Laziness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ops = set(x for x in dir(pl.DataFrame()) if not x.startswith('_'))\n",
    "lazy_ops = set(x for x in dir(pl.LazyFrame()) if not x.startswith('_'))\n",
    "len(sorted(df_ops - lazy_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lazy_ops & df_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts & Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "col(\"sample\")"
      ],
      "text/plain": [
       "<Expr ['col(\"sample\")'] at 0x155212510>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = pl.col('sample')\n",
    "col_ops = set(x for x in dir(col) if not x.startswith('_'))\n",
    "len(sorted(col_ops))\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "col(\"sample\").strict_cast(Int32).fill_null([col(\"sample\").mean()]).clip_max([dyn int: 100]).sample([dyn int: 10]).mean()"
      ],
      "text/plain": [
       "<Expr ['col(\"sample\").strict_cast(Int3…'] at 0x155212E10>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(col\n",
    " .cast(pl.Int32)\n",
    " .fill_null(col.mean())\n",
    " .clip(upper_bound=100)\n",
    " .sample(10)\n",
    " .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = col.cast(pl.Int64)\n",
    "ex2 = ex1.fill_null(col.cast(pl.Int64))\n",
    "ex3 = ex2.clip(upper_bound=100)\n",
    "ex4 = ex3.sample(10)\n",
    "ex5 = ex4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "col(\"a_column\")"
      ],
      "text/plain": [
       "<Expr ['col(\"a_column\")'] at 0x15471CDD0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.col.a_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'data/vehicles.csv.zip'\n",
    "import zipfile\n",
    "with zipfile.ZipFile(url) as z:\n",
    "    z.extractall('data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/vehicles.csv'\n",
    "df = pl.read_csv(path, null_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41_144, 83)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>barrels08</th><th>barrelsA08</th><th>charge120</th><th>charge240</th><th>city08</th><th>city08U</th><th>cityA08</th><th>cityA08U</th><th>cityCD</th><th>cityE</th><th>cityUF</th><th>co2</th><th>co2A</th><th>co2TailpipeAGpm</th><th>co2TailpipeGpm</th><th>comb08</th><th>comb08U</th><th>combA08</th><th>combA08U</th><th>combE</th><th>combinedCD</th><th>combinedUF</th><th>cylinders</th><th>displ</th><th>drive</th><th>engId</th><th>eng_dscr</th><th>feScore</th><th>fuelCost08</th><th>fuelCostA08</th><th>fuelType</th><th>fuelType1</th><th>ghgScore</th><th>ghgScoreA</th><th>highway08</th><th>highway08U</th><th>highwayA08</th><th>&hellip;</th><th>make</th><th>model</th><th>mpgData</th><th>phevBlended</th><th>pv2</th><th>pv4</th><th>range</th><th>rangeCity</th><th>rangeCityA</th><th>rangeHwy</th><th>rangeHwyA</th><th>trany</th><th>UCity</th><th>UCityA</th><th>UHighway</th><th>UHighwayA</th><th>VClass</th><th>year</th><th>youSaveSpend</th><th>guzzler</th><th>trans_dscr</th><th>tCharger</th><th>sCharger</th><th>atvType</th><th>fuelType2</th><th>rangeA</th><th>evMotor</th><th>mfrCode</th><th>c240Dscr</th><th>charge240b</th><th>c240bDscr</th><th>createdOn</th><th>modifiedOn</th><th>startStop</th><th>phevCity</th><th>phevHwy</th><th>phevComb</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>&hellip;</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>15.695714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>19</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>423.190476</td><td>21</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.0</td><td>&quot;Rear-Wheel Drive&quot;</td><td>9011</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>2000</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>25</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Alfa Romeo&quot;</td><td>&quot;Spider Veloce 2000&quot;</td><td>&quot;Y&quot;</td><td>false</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>23.3333</td><td>0.0</td><td>35.0</td><td>0.0</td><td>&quot;Two Seaters&quot;</td><td>1985</td><td>-2250</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>29.964545</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>807.909091</td><td>11</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>12</td><td>4.9</td><td>&quot;Rear-Wheel Drive&quot;</td><td>22020</td><td>&quot;(GUZZLER)&quot;</td><td>-1</td><td>3850</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>14</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Ferrari&quot;</td><td>&quot;Testarossa&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>11.0</td><td>0.0</td><td>19.0</td><td>0.0</td><td>&quot;Two Seaters&quot;</td><td>1985</td><td>-11500</td><td>&quot;T&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>12.207778</td><td>0.0</td><td>0.0</td><td>0.0</td><td>23</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>329.148148</td><td>27</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;Front-Wheel Drive&quot;</td><td>2100</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>1550</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>33</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Dodge&quot;</td><td>&quot;Charger&quot;</td><td>&quot;Y&quot;</td><td>false</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>29.0</td><td>0.0</td><td>47.0</td><td>0.0</td><td>&quot;Subcompact Cars&quot;</td><td>1985</td><td>0</td><td>null</td><td>&quot;SIL&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>29.964545</td><td>0.0</td><td>0.0</td><td>0.0</td><td>10</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>807.909091</td><td>11</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>8</td><td>5.2</td><td>&quot;Rear-Wheel Drive&quot;</td><td>2850</td><td>null</td><td>-1</td><td>3850</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>12</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Dodge&quot;</td><td>&quot;B150/B250 Wagon 2WD&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Automatic 3-spd&quot;</td><td>12.2222</td><td>0.0</td><td>16.6667</td><td>0.0</td><td>&quot;Vans&quot;</td><td>1985</td><td>-11500</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>17.347895</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>467.736842</td><td>19</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>66031</td><td>&quot;(FFS,TRBO)&quot;</td><td>-1</td><td>2700</td><td>0</td><td>&quot;Premium&quot;</td><td>&quot;Premium Gasoline&quot;</td><td>-1</td><td>-1</td><td>23</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>21.0</td><td>0.0</td><td>32.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-5750</td><td>null</td><td>null</td><td>&quot;T&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>14.982273</td><td>0.0</td><td>0.0</td><td>0.0</td><td>19</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>403.954545</td><td>22</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;Front-Wheel Drive&quot;</td><td>66030</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>1900</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>26</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Automatic 4-spd&quot;</td><td>24.0</td><td>0.0</td><td>37.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-1750</td><td>null</td><td>&quot;CLKUP&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>14.33087</td><td>0.0</td><td>0.0</td><td>0.0</td><td>20</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>386.391304</td><td>23</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;Front-Wheel Drive&quot;</td><td>66030</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>1850</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>28</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>25.0</td><td>0.0</td><td>39.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-1500</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>15.695714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>18</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>423.190476</td><td>21</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>66030</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>2000</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>24</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>&quot;Y&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Automatic 4-spd&quot;</td><td>23.0</td><td>0.0</td><td>34.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-2250</td><td>null</td><td>&quot;CLKUP&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>15.695714</td><td>0.0</td><td>0.0</td><td>0.0</td><td>18</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>423.190476</td><td>21</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>66030</td><td>&quot;(FFS)&quot;</td><td>-1</td><td>2000</td><td>0</td><td>&quot;Regular&quot;</td><td>&quot;Regular Gasoline&quot;</td><td>-1</td><td>-1</td><td>24</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>&quot;Y&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Manual 5-spd&quot;</td><td>23.0</td><td>0.0</td><td>34.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-2250</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr><tr><td>18.311667</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>-1</td><td>0.0</td><td>493.722222</td><td>18</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4</td><td>2.2</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>66031</td><td>&quot;(FFS,TRBO)&quot;</td><td>-1</td><td>2900</td><td>0</td><td>&quot;Premium&quot;</td><td>&quot;Premium Gasoline&quot;</td><td>-1</td><td>-1</td><td>21</td><td>0.0</td><td>0</td><td>&hellip;</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>&quot;N&quot;</td><td>false</td><td>0</td><td>90</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Automatic 4-spd&quot;</td><td>20.0</td><td>0.0</td><td>29.0</td><td>0.0</td><td>&quot;Compact Cars&quot;</td><td>1993</td><td>-6750</td><td>null</td><td>&quot;CLKUP&quot;</td><td>&quot;T&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>&quot;Tue Jan 01 00:00:00 EST 2013&quot;</td><td>null</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41_144, 83)\n",
       "┌───────────┬────────────┬───────────┬───────────┬───┬───────────┬──────────┬─────────┬──────────┐\n",
       "│ barrels08 ┆ barrelsA08 ┆ charge120 ┆ charge240 ┆ … ┆ startStop ┆ phevCity ┆ phevHwy ┆ phevComb │\n",
       "│ ---       ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---      ┆ ---     ┆ ---      │\n",
       "│ f64       ┆ f64        ┆ f64       ┆ f64       ┆   ┆ str       ┆ i64      ┆ i64     ┆ i64      │\n",
       "╞═══════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═════════╪══════════╡\n",
       "│ 15.695714 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 29.964545 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 12.207778 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 29.964545 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 17.347895 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ …         ┆ …          ┆ …         ┆ …         ┆ … ┆ …         ┆ …        ┆ …       ┆ …        │\n",
       "│ 14.982273 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 14.33087  ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 15.695714 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 15.695714 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "│ 18.311667 ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ null      ┆ 0        ┆ 0       ┆ 0        │\n",
       "└───────────┴────────────┴───────────┴───────────┴───┴───────────┴──────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy CSV Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy = pl.scan_csv(path, null_values=['NA'])\n",
    "print(lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lazy\n",
    "  .filter((pl.col('year') >= 1990) & (pl.col('year') < 2000))\n",
    "  .select(['year', 'make', 'model'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lazy\n",
    "  .filter((pl.col('year') >= 1990) & (pl.col('year') < 2000))\n",
    "  .select(['year', 'make', 'model'])\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Inference and Manual Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', 'drive',\n",
    "        'VClass', 'fuelType', 'barrels08', 'city08', 'highway08',\n",
    "        'createdOn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(pl.col(cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .select(pl.col(pl.Int64))\n",
    "  .describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "                pl.col('cylinders').cast(pl.UInt8),\n",
    "                pl.col('highway08').cast(pl.UInt8),\n",
    "                pl.col('city08').cast(pl.UInt8))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int8))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Type Shrinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols).shrink_dtype())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "                pl.col('cylinders').cast(pl.UInt8),\n",
    "                pl.col('highway08').cast(pl.UInt8),\n",
    "                pl.col('city08').cast(pl.UInt8))\n",
    "  .select(pl.col(pl.Float64))\n",
    "  .sample(n=10, seed=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "                pl.col('cylinders').cast(pl.UInt8),\n",
    "                pl.col('highway08').cast(pl.UInt8),\n",
    "                pl.col('city08').cast(pl.UInt8),\n",
    "                pl.col('displ').cast(pl.Float32),\n",
    "                pl.col('barrels08').cast(pl.Float32),\n",
    "                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Numbers from Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select('trany')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select('trany',\n",
    "         pl.col('trany')\n",
    "              .str.to_lowercase()\n",
    "              .str.contains('automatic') \n",
    "              .alias('automatic'))         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .with_columns('trany',\n",
    "         pl.col('trany')\n",
    "              .str.to_lowercase()\n",
    "              .str.contains('automatic') \n",
    "              .alias('is_automatic'))         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .with_columns('trany',\n",
    "         is_automatic=pl.col('trany')\n",
    "              .str.to_lowercase()\n",
    "              .str.contains('automatic')) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .group_by('trany')\n",
    " .len()\n",
    " .sort('len', descending=True)          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select(pl.col('trany')\n",
    "           .value_counts(sort=True))         \n",
    " .unnest('trany')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .select(pl.col('trany')).to_series()\n",
    "    .value_counts(sort=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .filter(pl.col('trany').is_null())\n",
    "  .select('year', 'make', 'model', 'VClass')          \n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select('trany',\n",
    "    is_automatic=pl.col('trany')\n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True)                 \n",
    "              )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select(num_gears=pl.col('trany')\n",
    "            .str.extract(r'(\\d+)')\n",
    "            .cast(pl.UInt8))\n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select(num_gears=pl.col('trany')\n",
    "            .str.extract(r'(\\d+)')\n",
    "            .cast(pl.UInt8)\n",
    "            .unique())\n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .select(num_gears=pl.col('trany')\n",
    "            .str.extract(r'(\\d+)')\n",
    "            .cast(pl.UInt8))\n",
    " .filter(pl.col('num_gears').is_null())          \n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    " .with_columns(\n",
    "    is_automatic=pl.col('trany')\n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True),             \n",
    "    num_gears=pl.col('trany')\n",
    "            .str.extract(r'(\\d+)')\n",
    "            .cast(pl.UInt8)\n",
    "            .fill_null(6)\n",
    "              ) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "                pl.col('cylinders').cast(pl.UInt8),\n",
    "                pl.col('highway08').cast(pl.UInt8),\n",
    "                pl.col('city08').cast(pl.UInt8),\n",
    "                pl.col('displ').cast(pl.Float32),\n",
    "                pl.col('barrels08').cast(pl.Float32),\n",
    "                )\n",
    "  .select(pl.col(pl.String))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       is_automatic=pl.col('trany')\n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True),\n",
    "       num_gears=pl.col('trany')\n",
    "             .str.extract(r'(\\d+)')\n",
    "             .cast(pl.UInt8)\n",
    "             .fill_null(6))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       is_automatic=pl.col('trany')\n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True),\n",
    "       num_gears=pl.col('trany')\n",
    "             .str.extract(r'(\\d+)')\n",
    "             .cast(pl.UInt8)\n",
    "             .fill_null(6))    \n",
    "  .select(pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_auto(df):\n",
    "   cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', \n",
    "          'drive', 'VClass', 'fuelType', 'barrels08', 'city08', \n",
    "          'highway08', 'createdOn']\n",
    "   return (df\n",
    "    .select(pl.col(cols))\n",
    "    .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       is_automatic=pl.col('trany')                    \n",
    "              .str.contains('Automatic')\n",
    "              .fill_null('Automatic'),\n",
    "       num_gears=pl.col('trany')\n",
    "             .str.extract(r'(\\d+)')\n",
    "             .cast(pl.UInt8)\n",
    "             .fill_null(6))    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweak_auto(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweak_auto(df).estimated_size(unit='mb')\n",
    "# 2.446714401245117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(cols).estimated_size(unit='mb')\n",
    "# 5.914606094360352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweak_auto(df).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "print(tweak_auto(df)\n",
    " .select(cs.numeric())\n",
    " .corr()\n",
    " .pipe(lambda df_: df_.insert_column(0, pl.Series('names', df_.columns)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being Lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "path = 'data/vehicles.csv'\n",
    "lazy = pl.scan_csv(path, null_values=['NA'])\n",
    "\n",
    "def tweak_auto(df):\n",
    "   cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', \n",
    "          'drive', 'VClass', 'fuelType', 'barrels08', 'city08', \n",
    "          'highway08', 'createdOn']\n",
    "   return (df\n",
    "    .select(pl.col(cols))\n",
    "    .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       is_automatic=pl.col('trany')                    \n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True),\n",
    "       num_gears=pl.col('trany')\n",
    "             .str.extract(r'(\\d+)')\n",
    "             .cast(pl.UInt8)\n",
    "             .fill_null(6))    \n",
    "    )\n",
    "                      \n",
    "print(tweak_auto(lazy).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Functions and Methods\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Data Manipulation with Polars Using the Fuel Economy Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "path = 'data/vehicles.csv'\n",
    "raw = pl.read_csv(path, null_values=['NA'])\n",
    "\n",
    "def tweak_auto(df):\n",
    "   cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', \n",
    "          'drive', 'VClass', 'fuelType', 'barrels08', 'city08', \n",
    "          'highway08', 'createdOn']\n",
    "   return (df\n",
    "    .select(pl.col(cols))\n",
    "    .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', \n",
    "               'fuelType']).cast(pl.Categorical),\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       is_automatic=pl.col('trany')                    \n",
    "              .str.contains('Automatic')\n",
    "              .fill_null(True),\n",
    "       num_gears=pl.col('trany')\n",
    "             .str.extract(r'(\\d+)')\n",
    "             .cast(pl.UInt8)\n",
    "             .fill_null(6))    \n",
    "    )\n",
    "\n",
    "autos = tweak_auto(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41_144, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year</th><th>make</th><th>model</th><th>displ</th><th>cylinders</th><th>trany</th><th>drive</th><th>VClass</th><th>fuelType</th><th>barrels08</th><th>city08</th><th>highway08</th><th>createdOn</th><th>is_automatic</th><th>num_gears</th></tr><tr><td>i16</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>str</td><td>cat</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>u8</td><td>datetime[μs]</td><td>bool</td><td>u8</td></tr></thead><tbody><tr><td>1985</td><td>&quot;Alfa Romeo&quot;</td><td>&quot;Spider Veloce 2000&quot;</td><td>2.0</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>19</td><td>25</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1985</td><td>&quot;Ferrari&quot;</td><td>&quot;Testarossa&quot;</td><td>4.9</td><td>12</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>9</td><td>14</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;Charger&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Subcompact Cars&quot;</td><td>&quot;Regular&quot;</td><td>12.207778</td><td>23</td><td>33</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;B150/B250 Wagon 2WD&quot;</td><td>5.2</td><td>8</td><td>&quot;Automatic 3-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Vans&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>10</td><td>12</td><td>2013-01-01 00:00:00</td><td>true</td><td>3</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>17.347895</td><td>17</td><td>23</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.982273</td><td>19</td><td>26</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.33087</td><td>20</td><td>28</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>18.311666</td><td>16</td><td>21</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41_144, 15)\n",
       "┌──────┬────────────┬──────────────┬───────┬───┬───────────┬─────────────┬─────────────┬───────────┐\n",
       "│ year ┆ make       ┆ model        ┆ displ ┆ … ┆ highway08 ┆ createdOn   ┆ is_automati ┆ num_gears │\n",
       "│ ---  ┆ ---        ┆ ---          ┆ ---   ┆   ┆ ---       ┆ ---         ┆ c           ┆ ---       │\n",
       "│ i16  ┆ cat        ┆ cat          ┆ f32   ┆   ┆ u8        ┆ datetime[μs ┆ ---         ┆ u8        │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ ]           ┆ bool        ┆           │\n",
       "╞══════╪════════════╪══════════════╪═══════╪═══╪═══════════╪═════════════╪═════════════╪═══════════╡\n",
       "│ 1985 ┆ Alfa Romeo ┆ Spider       ┆ 2.0   ┆ … ┆ 25        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆ Veloce 2000  ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1985 ┆ Ferrari    ┆ Testarossa   ┆ 4.9   ┆ … ┆ 14        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1985 ┆ Dodge      ┆ Charger      ┆ 2.2   ┆ … ┆ 33        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1985 ┆ Dodge      ┆ B150/B250    ┆ 5.2   ┆ … ┆ 12        ┆ 2013-01-01  ┆ true        ┆ 3         │\n",
       "│      ┆            ┆ Wagon 2WD    ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 23        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆ Turbo        ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ …    ┆ …          ┆ …            ┆ …     ┆ … ┆ …         ┆ …           ┆ …           ┆ …         │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy       ┆ 2.2   ┆ … ┆ 26        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy       ┆ 2.2   ┆ … ┆ 28        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 24        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 24        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│      ┆            ┆              ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 21        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│      ┆            ┆ Turbo        ┆       ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "└──────┴────────────┴──────────────┴───────┴───┴───────────┴─────────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>is_automatic</th><th>count</th></tr><tr><td>bool</td><td>u32</td></tr></thead><tbody><tr><td>true</td><td>28335</td></tr><tr><td>false</td><td>12809</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────────────┬───────┐\n",
       "│ is_automatic ┆ count │\n",
       "│ ---          ┆ ---   │\n",
       "│ bool         ┆ u32   │\n",
       "╞══════════════╪═══════╡\n",
       "│ true         ┆ 28335 │\n",
       "│ false        ┆ 12809 │\n",
       "└──────────────┴───────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos['is_automatic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41_144, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>highway08</th><th>city08</th><th>mpg_ratio</th></tr><tr><td>u8</td><td>u8</td><td>f64</td></tr></thead><tbody><tr><td>25</td><td>19</td><td>1.315789</td></tr><tr><td>14</td><td>9</td><td>1.555556</td></tr><tr><td>33</td><td>23</td><td>1.434783</td></tr><tr><td>12</td><td>10</td><td>1.2</td></tr><tr><td>23</td><td>17</td><td>1.352941</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>26</td><td>19</td><td>1.368421</td></tr><tr><td>28</td><td>20</td><td>1.4</td></tr><tr><td>24</td><td>18</td><td>1.333333</td></tr><tr><td>24</td><td>18</td><td>1.333333</td></tr><tr><td>21</td><td>16</td><td>1.3125</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41_144, 3)\n",
       "┌───────────┬────────┬───────────┐\n",
       "│ highway08 ┆ city08 ┆ mpg_ratio │\n",
       "│ ---       ┆ ---    ┆ ---       │\n",
       "│ u8        ┆ u8     ┆ f64       │\n",
       "╞═══════════╪════════╪═══════════╡\n",
       "│ 25        ┆ 19     ┆ 1.315789  │\n",
       "│ 14        ┆ 9      ┆ 1.555556  │\n",
       "│ 33        ┆ 23     ┆ 1.434783  │\n",
       "│ 12        ┆ 10     ┆ 1.2       │\n",
       "│ 23        ┆ 17     ┆ 1.352941  │\n",
       "│ …         ┆ …      ┆ …         │\n",
       "│ 26        ┆ 19     ┆ 1.368421  │\n",
       "│ 28        ┆ 20     ┆ 1.4       │\n",
       "│ 24        ┆ 18     ┆ 1.333333  │\n",
       "│ 24        ┆ 18     ┆ 1.333333  │\n",
       "│ 21        ┆ 16     ┆ 1.3125    │\n",
       "└───────────┴────────┴───────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(autos\n",
    "  .select(pl.col(['highway08', 'city08']),\n",
    "          mpg_ratio=(pl.col('highway08') / pl.col('city08')))\n",
    "          \n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41_144, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year</th><th>make</th><th>model</th><th>displ</th><th>cylinders</th><th>trany</th><th>drive</th><th>VClass</th><th>fuelType</th><th>barrels08</th><th>city08</th><th>highway08</th><th>createdOn</th><th>is_automatic</th><th>num_gears</th><th>mpg_ratio</th></tr><tr><td>i16</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>str</td><td>cat</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>u8</td><td>datetime[μs]</td><td>bool</td><td>u8</td><td>f64</td></tr></thead><tbody><tr><td>1985</td><td>&quot;Alfa Romeo&quot;</td><td>&quot;Spider Veloce 2000&quot;</td><td>2.0</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>19</td><td>25</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.315789</td></tr><tr><td>1985</td><td>&quot;Ferrari&quot;</td><td>&quot;Testarossa&quot;</td><td>4.9</td><td>12</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>9</td><td>14</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.555556</td></tr><tr><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;Charger&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Subcompact Cars&quot;</td><td>&quot;Regular&quot;</td><td>12.207778</td><td>23</td><td>33</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.434783</td></tr><tr><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;B150/B250 Wagon 2WD&quot;</td><td>5.2</td><td>8</td><td>&quot;Automatic 3-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Vans&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>10</td><td>12</td><td>2013-01-01 00:00:00</td><td>true</td><td>3</td><td>1.2</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>17.347895</td><td>17</td><td>23</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.352941</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.982273</td><td>19</td><td>26</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td><td>1.368421</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.33087</td><td>20</td><td>28</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.4</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td><td>1.333333</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td><td>1.333333</td></tr><tr><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>18.311666</td><td>16</td><td>21</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td><td>1.3125</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41_144, 16)\n",
       "┌──────┬────────────┬──────────────┬───────┬───┬─────────────┬─────────────┬───────────┬───────────┐\n",
       "│ year ┆ make       ┆ model        ┆ displ ┆ … ┆ createdOn   ┆ is_automati ┆ num_gears ┆ mpg_ratio │\n",
       "│ ---  ┆ ---        ┆ ---          ┆ ---   ┆   ┆ ---         ┆ c           ┆ ---       ┆ ---       │\n",
       "│ i16  ┆ cat        ┆ cat          ┆ f32   ┆   ┆ datetime[μs ┆ ---         ┆ u8        ┆ f64       │\n",
       "│      ┆            ┆              ┆       ┆   ┆ ]           ┆ bool        ┆           ┆           │\n",
       "╞══════╪════════════╪══════════════╪═══════╪═══╪═════════════╪═════════════╪═══════════╪═══════════╡\n",
       "│ 1985 ┆ Alfa Romeo ┆ Spider       ┆ 2.0   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.315789  │\n",
       "│      ┆            ┆ Veloce 2000  ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1985 ┆ Ferrari    ┆ Testarossa   ┆ 4.9   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.555556  │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1985 ┆ Dodge      ┆ Charger      ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.434783  │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1985 ┆ Dodge      ┆ B150/B250    ┆ 5.2   ┆ … ┆ 2013-01-01  ┆ true        ┆ 3         ┆ 1.2       │\n",
       "│      ┆            ┆ Wagon 2WD    ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.352941  │\n",
       "│      ┆            ┆ Turbo        ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ …    ┆ …          ┆ …            ┆ …     ┆ … ┆ …           ┆ …           ┆ …         ┆ …         │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy       ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ true        ┆ 4         ┆ 1.368421  │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy       ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.4       │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ true        ┆ 4         ┆ 1.333333  │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ false       ┆ 5         ┆ 1.333333  │\n",
       "│      ┆            ┆              ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "│ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ 2.2   ┆ … ┆ 2013-01-01  ┆ true        ┆ 4         ┆ 1.3125    │\n",
       "│      ┆            ┆ Turbo        ┆       ┆   ┆ 00:00:00    ┆             ┆           ┆           │\n",
       "└──────┴────────────┴──────────────┴───────┴───┴─────────────┴─────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(autos\n",
    "  .with_columns(mpg_ratio=pl.col('highway08') / pl.col('city08'))\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41_144, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>year</th><th>make</th><th>model</th><th>displ</th><th>cylinders</th><th>trany</th><th>drive</th><th>VClass</th><th>fuelType</th><th>barrels08</th><th>city08</th><th>highway08</th><th>createdOn</th><th>is_automatic</th><th>num_gears</th></tr><tr><td>u32</td><td>i16</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>str</td><td>cat</td><td>cat</td><td>cat</td><td>f32</td><td>u8</td><td>u8</td><td>datetime[μs]</td><td>bool</td><td>u8</td></tr></thead><tbody><tr><td>0</td><td>1985</td><td>&quot;Alfa Romeo&quot;</td><td>&quot;Spider Veloce 2000&quot;</td><td>2.0</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>19</td><td>25</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>1</td><td>1985</td><td>&quot;Ferrari&quot;</td><td>&quot;Testarossa&quot;</td><td>4.9</td><td>12</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Two Seaters&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>9</td><td>14</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>2</td><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;Charger&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Subcompact Cars&quot;</td><td>&quot;Regular&quot;</td><td>12.207778</td><td>23</td><td>33</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>3</td><td>1985</td><td>&quot;Dodge&quot;</td><td>&quot;B150/B250 Wagon 2WD&quot;</td><td>5.2</td><td>8</td><td>&quot;Automatic 3-spd&quot;</td><td>&quot;Rear-Wheel Drive&quot;</td><td>&quot;Vans&quot;</td><td>&quot;Regular&quot;</td><td>29.964546</td><td>10</td><td>12</td><td>2013-01-01 00:00:00</td><td>true</td><td>3</td></tr><tr><td>4</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>17.347895</td><td>17</td><td>23</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>41139</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.982273</td><td>19</td><td>26</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr><tr><td>41140</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;Front-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>14.33087</td><td>20</td><td>28</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>41141</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr><tr><td>41142</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD&quot;</td><td>2.2</td><td>4</td><td>&quot;Manual 5-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Regular&quot;</td><td>15.695714</td><td>18</td><td>24</td><td>2013-01-01 00:00:00</td><td>false</td><td>5</td></tr><tr><td>41143</td><td>1993</td><td>&quot;Subaru&quot;</td><td>&quot;Legacy AWD Turbo&quot;</td><td>2.2</td><td>4</td><td>&quot;Automatic 4-spd&quot;</td><td>&quot;4-Wheel or All-Wheel Drive&quot;</td><td>&quot;Compact Cars&quot;</td><td>&quot;Premium&quot;</td><td>18.311666</td><td>16</td><td>21</td><td>2013-01-01 00:00:00</td><td>true</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41_144, 16)\n",
       "┌───────┬──────┬────────────┬──────────────┬───┬───────────┬─────────────┬─────────────┬───────────┐\n",
       "│ index ┆ year ┆ make       ┆ model        ┆ … ┆ highway08 ┆ createdOn   ┆ is_automati ┆ num_gears │\n",
       "│ ---   ┆ ---  ┆ ---        ┆ ---          ┆   ┆ ---       ┆ ---         ┆ c           ┆ ---       │\n",
       "│ u32   ┆ i16  ┆ cat        ┆ cat          ┆   ┆ u8        ┆ datetime[μs ┆ ---         ┆ u8        │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ ]           ┆ bool        ┆           │\n",
       "╞═══════╪══════╪════════════╪══════════════╪═══╪═══════════╪═════════════╪═════════════╪═══════════╡\n",
       "│ 0     ┆ 1985 ┆ Alfa Romeo ┆ Spider       ┆ … ┆ 25        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆ Veloce 2000  ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 1     ┆ 1985 ┆ Ferrari    ┆ Testarossa   ┆ … ┆ 14        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 2     ┆ 1985 ┆ Dodge      ┆ Charger      ┆ … ┆ 33        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 3     ┆ 1985 ┆ Dodge      ┆ B150/B250    ┆ … ┆ 12        ┆ 2013-01-01  ┆ true        ┆ 3         │\n",
       "│       ┆      ┆            ┆ Wagon 2WD    ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 4     ┆ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ … ┆ 23        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆ Turbo        ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ …     ┆ …    ┆ …          ┆ …            ┆ … ┆ …         ┆ …           ┆ …           ┆ …         │\n",
       "│ 41139 ┆ 1993 ┆ Subaru     ┆ Legacy       ┆ … ┆ 26        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 41140 ┆ 1993 ┆ Subaru     ┆ Legacy       ┆ … ┆ 28        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 41141 ┆ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ … ┆ 24        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 41142 ┆ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ … ┆ 24        ┆ 2013-01-01  ┆ false       ┆ 5         │\n",
       "│       ┆      ┆            ┆              ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "│ 41143 ┆ 1993 ┆ Subaru     ┆ Legacy AWD   ┆ … ┆ 21        ┆ 2013-01-01  ┆ true        ┆ 4         │\n",
       "│       ┆      ┆            ┆ Turbo        ┆   ┆           ┆ 00:00:00    ┆             ┆           │\n",
       "└───────┴──────┴────────────┴──────────────┴───┴───────────┴─────────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(autos\n",
    " .with_row_index('index')\n",
    ")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .drop('createdOn')\n",
    " .columns\n",
    ")\n",
    "['year', 'make', 'model', 'displ', 'cylinders', 'trany', 'drive',\n",
    "     'VClass', 'fuelType', 'barrels08', 'city08', 'highway08',\n",
    "     'is_automatic', 'num_gears']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', \n",
    "  'drive', 'VClass', 'fuelType', 'barrels08', 'city08', 'highway08', \n",
    "  'is_automatic', 'num_gears']\n",
    "\n",
    "print(autos\n",
    " .select(final_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .select(pl.all()\n",
    "           .exclude(['createdOn', 'barrels08'])\n",
    "           .name.suffix('_auto'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['year', 'make', 'model', 'city_mpg', 'highway_mpg']\n",
    "print(autos\n",
    " .with_columns(pl.col('city08').alias('city_mpg'),\n",
    "               highway_mpg=pl.col('highway08'))\n",
    " .select(final_cols)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(highway_mpg=pl.col('highway08'),\n",
    "               pl.col('city08').alias('city_mpg'))\n",
    " .select(final_cols)\n",
    " )\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# SyntaxError: positional argument follows keyword argument\n",
    "#      (3173993774.py, line 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['year', 'make', 'model', 'city_mpg', 'highway_mpg']\n",
    "print(autos\n",
    " .rename({'city08':'city_mpg',\n",
    "          'highway08':'highway_mpg'})\n",
    " .select(final_cols)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks = pl.DataFrame(\n",
    "   {'make': ['Ford', 'Tesla', 'Chevy', 'Custom', 'Ford'],\n",
    "    'model': ['F150', 'Cybertruck', 'Silverado', 'HotRod', 'F250'],\n",
    "    'year': [2018, 2024, 2019, 1967, 2017],\n",
    "    'city_mpg': [19, None, 17, 12, 18],\n",
    "   })\n",
    "\n",
    "print(trucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer = pl.DataFrame(\n",
    "   {'name': ['Ford', 'Tesla', 'Chevy', 'Toyota'],\n",
    "    'country': ['USA', 'USA', 'USA', 'Japan'],\n",
    "    'founded': [1903, 2003, 1911, 1937],\n",
    "    'employees': [199_000, 48_000, 225_000, 370_000],\n",
    "    'vehicles': [80, 3, 45, 30],\n",
    "    })\n",
    "\n",
    "print(manufacturer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, how='left'))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ValueError: must specify `on` OR `left_on` and `right_on`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, how='left', left_on='name', \n",
    "                        right_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trucks.join(manufacturer, how='left', right_on='name', \n",
    "                  left_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, left_on='name', right_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, how='outer', left_on='name', \n",
    "                        right_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, how='semi', left_on='name', \n",
    "                        right_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = pl.DataFrame({'size': ['small', 'medium', 'large'],})\n",
    "colors = pl.DataFrame({'color': ['red', 'green', ],})\n",
    "print(sizes.join(colors, how='cross'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, how='anti', left_on='name', \n",
    "                        right_on='make'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trucks.join(manufacturer, how='anti', right_on='name', \n",
    "                  left_on='make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, left_on='name', right_on='make', \n",
    "                        validate='1:1'))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ComputeError: the join keys did not fulfil 1:1 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, left_on='name', right_on='make', \n",
    "                        validate='m:1'))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ComputeError: the join keys did not fulfil m:1 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manufacturer.join(trucks, left_on='name', right_on='make',\n",
    "                        validate='1:m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trucks\n",
    "  .filter(pl.col('make').is_duplicated())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up Joins with Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.openei.org/files/907/'\\\n",
    "   '2016cityandcountylightdutyvehicleinventory.xlsb'\n",
    "inv_raw = pl.read_excel(url, engine='calamine', \n",
    "    read_options=dict(header_row=1), sheet_name='City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inv_raw\n",
    "  .select(['state_abbr', 'gisjoin', 'city_id', 'city_name',\n",
    "           'fuel_type_org', 'fuel_type', 'class', '2000', '2001'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(i) for i in range(2000, 2019)]\n",
    "\n",
    "inv_yr = (inv_raw\n",
    "    .with_columns(\n",
    "        [pl.col(year).replace('', 0).cast(pl.Float32) \n",
    "         for year in years]) \n",
    "    .melt(variable_name='year', value_vars=years,\n",
    "          id_vars=['state_abbr', 'gisjoin', 'city_id', 'city_name',\n",
    "                   'fuel_type_org', 'fuel_type', 'class',]\n",
    "    )\n",
    "    .select('state_abbr', 'city_name', 'fuel_type_org', 'fuel_type', 'class', \n",
    "            year=pl.col('year').cast(pl.Int16),\n",
    "            percent=pl.col('value')*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inv_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple cat\n",
    "gas_mapping = {\n",
    "    'Diesel': 'Diesel vehicle',\n",
    "    'Regular': 'Gasoline vehicle',\n",
    "    'Premium': 'Gasoline vehicle',\n",
    "    'Midgrade': 'Gasoline vehicle',\n",
    "    'Gasoline or E85': 'Flex fuel vehicle',\n",
    "    'Premium or E85': 'Flex fuel vehicle',\n",
    "    'Premium Gas or Electricity': 'Plug-in hybrid electric vehicle',\n",
    "    'Regular Gas or Electricity': 'Plug-in hybrid electric vehicle',\n",
    "    'Premium and Electricity': 'Hybrid electric vehicle',\n",
    "    'Regular Gas and Electricity': 'Hybrid electric vehicle',\n",
    "    'Electricity': 'Electric vehicle',\n",
    "    'Gasoline or natural gas': 'Other/Unknown',\n",
    "    'Gasoline or propane': 'Other/Unknown',\n",
    "    'CNG': 'Other/Unknown',\n",
    "\n",
    "}\n",
    "agg_yr = (autos\n",
    " .with_columns(VClass=pl.col('VClass').cast(pl.String))\n",
    " .with_columns(\n",
    "    simple_class=pl.when(pl.col('VClass')\n",
    "                         .str.to_lowercase().str.contains('car'))\n",
    "                        .then(pl.lit('Car'))\n",
    "                   .when(pl.col('VClass')\n",
    "                         .str.to_lowercase().str.contains('truck'))\n",
    "                        .then(pl.lit('Truck'))\n",
    "                   .otherwise(pl.lit('Other')),\n",
    "    fuel_type=pl.col('fuelType').cast(pl.String)\n",
    "                .replace(gas_mapping, default='Missing'))\n",
    " .group_by(['year', 'simple_class', 'fuel_type'])\n",
    " .agg(pl.col('city08').mean().alias('mean_mpg'))                                    \n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agg_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (agg_yr\n",
    " .join(inv_yr, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_yr_shuf = agg_yr.sample(len(agg_yr), with_replacement=False, seed=42)      \n",
    "inv_yr_shuf = inv_yr.sample(len(inv_yr), with_replacement=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(agg_yr_shuf\n",
    " .join(inv_yr_shuf, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by year\n",
    "agg_yr_sort = agg_yr.sort('year')\n",
    "inv_yr_sort = inv_yr.sort('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(agg_yr_sort\n",
    " .join(inv_yr_sort, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by year and class\n",
    "agg_yr_sort2 = agg_yr.sort('year', 'simple_class', 'fuel_type')\n",
    "inv_yr_sort2 = inv_yr.sort('year', 'class', 'fuel_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(agg_yr_sort2\n",
    " .join(inv_yr_sort2, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.StringCache():\n",
    "    agg_yr_cat = agg_yr_sort2.with_columns(\n",
    "        pl.col('simple_class', 'fuel_type').cast(pl.Categorical))\n",
    "    inv_yr_cat = inv_yr_sort2.with_columns(\n",
    "        pl.col('class', 'fuel_type').cast(pl.Categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(agg_yr_cat\n",
    " .join(inv_yr_cat, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pd = agg_yr_sort2.to_pandas().sort_values(['year', 'simple_class', \n",
    "                                               'fuel_type'])\n",
    "inv_pd = inv_yr_sort2.to_pandas().sort_values(['year', 'class', \n",
    "                                               'fuel_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(agg_pd\n",
    " .merge(inv_pd, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "        right_on=['year', 'class', 'fuel_type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agg_yr\n",
    " .join(inv_yr, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    " .filter(city_name='Salt Lake City')\n",
    " .pivot(index='year', columns='simple_class', values='mean_mpg', \n",
    "        aggregate_function='mean')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(agg_yr\n",
    " .join(inv_yr, left_on=['year', 'simple_class', 'fuel_type'], \n",
    "       right_on=['year', 'class', 'fuel_type'])\n",
    " .filter(city_name='Salt Lake City')\n",
    " .pivot(index='year', columns='simple_class', values='mean_mpg', \n",
    "        aggregate_function='mean')\n",
    " .plot.line(x='year', y=['Car', 'Truck'], \n",
    "            title='Salt Lake City Vehicle Mileage')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# :NdOverlay   [Variable]\n",
    "#    :Curve   [year]   (value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .tail(10)\n",
    " .vstack(autos.head(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping and Pivoting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = (autos\n",
    " .group_by('make')\n",
    " .agg(pl.col('city08').count())\n",
    " .sort('city08')\n",
    " .tail(5)\n",
    ")\n",
    "\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.col('make').is_in(top_n['make']))     \n",
    " .pivot(index='year', columns='make', values='city08', \n",
    "        aggregate_function='median')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = (autos\n",
    " .filter(pl.col('make').is_in(top_n['make']))     \n",
    " .pivot(index='year', columns='make', values='city08', \n",
    "        aggregate_function='median')\n",
    " .sort('year')\n",
    ")\n",
    "\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = (autos\n",
    " .filter(pl.col('make').is_in(top_n['make']))\n",
    " .sort('year')\n",
    " .set_sorted('year')\n",
    " .pivot(index='year', columns='make', values='city08', \n",
    "        aggregate_function='median')\n",
    ")\n",
    "\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = (autos\n",
    " .filter(pl.col('make').is_in(top_n['make']))\n",
    " .sort('year')\n",
    " .pivot(index='year', columns='make', values='city08', \n",
    "        aggregate_function='median', maintain_order=True)\n",
    ")\n",
    "\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivoted\n",
    " .melt(id_vars='year', value_vars=['Chevrolet', 'Ford', 'GMC', \n",
    "                                   'Toyota', 'Dodge'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivoted\n",
    " .melt(id_vars='year', value_vars=['Chevrolet', 'Ford', 'GMC', \n",
    "                                   'Toyota', 'Dodge'],\n",
    "       value_name='median_city_mpg', variable_name='make')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.col('highway08').is_duplicated())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.col('highway08').is_duplicated())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.struct(pl.col('year'), pl.col('model')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.struct(pl.col('year'), pl.col('model')).is_duplicated())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.struct(pl.col('year'), pl.col('model')).is_duplicated())\n",
    " .sort('year', 'model')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pl.DataFrame({'val': [-1.1, 0, 2.3, None, 5.7, 7]})\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_df\n",
    " .with_columns(val2=pl.col('val')/pl.col('val'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_df\n",
    " .with_columns(val2=pl.col('val')/pl.col('val'))\n",
    " .with_columns(is_null2=pl.col('val2').is_null(),\n",
    "               is_nan2=pl.col('val2').is_nan(),\n",
    "               is_finite2=pl.col('val2').is_finite(),\n",
    "               interpolate=pl.col('val2').interpolate()\n",
    "  )   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_df\n",
    " .with_columns(\n",
    "               forward=pl.col('val').fill_null(strategy='forward'),\n",
    "               backward=pl.col('val').fill_null(strategy='backward'),\n",
    "               min=pl.col('val').fill_null(strategy='min'),\n",
    "               max=pl.col('val').fill_null(strategy='max')\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_df\n",
    " .with_columns(\n",
    "               mean=pl.col('val').fill_null(pl.col('val').mean()),\n",
    "               interpolate=pl.col('val').interpolate(),\n",
    "               nearest=pl.col('val').interpolate('nearest')\n",
    "               )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(missing_df\n",
    " .with_columns(val2=pl.col('val')/pl.col('val'))\n",
    " .write_csv('missing.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%pfile missing.csv\n",
    "val,val2\n",
    "-1.1,1.0\n",
    "0.0,NaN\n",
    "2.3,1.0\n",
    ",\n",
    "5.7,1.0\n",
    "7.0,1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.read_csv('missing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_df\n",
    " .with_columns(val2=pl.col('val')/pl.col('val'))\n",
    " .to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third-Party Libraries and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (missing_df\n",
    " .with_columns(val2=pl.col('val')/pl.col('val')))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pl.Series(range(6))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "reg = xgb.XGBRegressor()\n",
    "reg.fit(X, y)\n",
    "# XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "#              colsample_bylevel=None, colsample_bynode=None,\n",
    "#              colsample_bytree=None, device=None,\n",
    "#      early_stopping_rounds=None,\n",
    "#              enable_categorical=False, eval_metric=None,\n",
    "#      feature_types=None,\n",
    "#              gamma=None, grow_policy=None, importance_type=None,\n",
    "#              interaction_constraints=None, learning_rate=None,\n",
    "#      max_bin=None,\n",
    "#              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "#              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "#              min_child_weight=None, missing=nan,\n",
    "#      monotone_constraints=None,\n",
    "#              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
    "#              num_parallel_tree=None, random_state=None, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Column Selectors and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".select(cs.numeric().is_null().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dates_or_bools = (cs.all() - cs.date() - cs.boolean())\n",
    "print(no_dates_or_bools)\n",
    "# SELECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cs.expand_selector(autos, no_dates_or_bools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".select(pl.col('displ').mean().over(['year', 'make']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".with_columns(\n",
    "   pl.col('displ').mean().over(['year', 'make']).alias('mean_displ'))\n",
    ".filter(pl.col('displ').is_null())\n",
    ".select(['year', 'make', 'displ', 'mean_displ'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_displ = (pl.col('displ')\n",
    "              .mean().over(['year', 'make'])\n",
    "              .fill_null(0)\n",
    "              .alias('mean_displ')\n",
    ")\n",
    "print(autos\n",
    ".with_columns(mean_displ)\n",
    ".filter(pl.col('displ').is_null())\n",
    ".select(['year', 'make', 'displ', 'mean_displ'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".with_columns(pl.col('displ').fill_null(mean_displ))\n",
    ".select(['year', 'make', 'model', 'displ'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(thing):\n",
    "  print(type(thing))\n",
    "  return thing\n",
    "\n",
    "print(autos\n",
    " .select(pl.col('make').map_batches(debug))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def standardize(arr_pl):\n",
    "    arr = arr_pl.to_numpy()\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\n",
    "\n",
    "print(autos\n",
    " .select(city08_standardized=pl.col('city08').map_batches(standardize))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_pl(col):\n",
    "  return (col - col.mean()) / col.std()\n",
    "\n",
    "print(autos\n",
    " .select(city08_standardized=standardize_pl(pl.col('city08')))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Functions and Methods\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# String Manipulation\n",
    "\n",
    "## Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/' \\\n",
    "  'master/data/__mharrison__2020-2021.csv'\n",
    "import polars as pl\n",
    "raw = pl.read_csv(url)\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_twit(df):\n",
    "    return (df\n",
    "       .select(['Tweet id', 'Tweet permalink', 'Tweet text', 'time', \n",
    "                'impressions', 'engagements', 'engagement rate',\n",
    "                'retweets', 'replies', 'likes', 'user profile clicks'])\n",
    "    )\n",
    "\n",
    "twit = tweak_twit(raw)\n",
    "print(twit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pl.col('Tweet permalink')\n",
    "print([m for m in dir(col.str)\n",
    "    if not m.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted([m for m in \n",
    "      set(dir(col.str)) & set(dir(''))\n",
    "      if not m.startswith('_')]))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted([m for m in \n",
    "      set(dir(col.str)) - set(dir(''))\n",
    "      if not m.startswith('_')]))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://metasnake.com'.startswith('https://twitter.com')\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .filter(~col.str.starts_with('https://twitter.com/'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://metasnake.com/effective-polars'.split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([m for m in dir(col.list)\n",
    "    if not m.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.len())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.to_struct())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.to_struct())\n",
    " .unnest('Tweet permalink')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.to_struct())\n",
    " .unnest('Tweet permalink')\n",
    " .to_struct()          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.join('/')\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(col.str.split('/')\n",
    "    .list.to_struct())\n",
    " .select(pl.all().map_elements(lambda elem: list(elem)))          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .with_columns(username=col.str.split('/')\n",
    "    .list.get(3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .with_columns(username=col.str.split('/')\n",
    "    .list[3])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Username with a Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'^https:\\/\\/twitter\\.com\\/([a-zA-Z0-9_]+)\\/status\\/(\\d+)$'\n",
    "print(twit\n",
    " .select(user=col.str.extract(regex, group_index=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words and Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_col = pl.col('Tweet text')\n",
    "print(twit\n",
    " .select(tweet_col.str.split(' '))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(tweet_col.str.split(' ').list.len())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .with_columns(word_count=tweet_col.str.split(' ').list.len())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(tweet_col.str.split(' ').list.eval(\n",
    "    pl.element().str.starts_with('@')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(tweet_col.str.split(' ').str.starts_with('@'))\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# InvalidOperationError: cannot cast List type (inner: 'String', to:\n",
    "#      'String')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(tweet_col.str.split(' ')\n",
    "         .list.eval(pl.element().str.starts_with('@'))\n",
    "         .list.sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(tweet_col.str.split(' ')\n",
    "         .list.eval(pl.element()\n",
    "                    .str.starts_with('@')\n",
    "                    .list.len()))\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# SchemaError: invalid series dtype: expected `List`, got `bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .with_columns(word_count=tweet_col.str.split(' ').list.len(),\n",
    "     num_mentions=tweet_col.str.split(' ')\n",
    "                   .list.eval(pl.element().str.starts_with('@'))\n",
    "                   .list.sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ascii = r'[^\\x00-\\x7F]'\n",
    "tweet_col = pl.col('Tweet text')\n",
    "print(twit\n",
    " .select(tweet_col, has_emoji=tweet_col.str.contains(non_ascii))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select('engagements', \n",
    "     tweet_len=tweet_col.str.split(' ').list.len(),\n",
    "     has_emoji=tweet_col.str.contains(non_ascii))\n",
    " .corr()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def jitter(col, amount=.5):\n",
    "    return col + np.random.uniform(-amount, amount, len(col))   \n",
    "\n",
    "(twit\n",
    " .select('engagements', \n",
    "     has_emoji=tweet_col.str.contains(non_ascii).cast(pl.Int8))\n",
    " .pipe(lambda df: df.with_columns(jitter(df['has_emoji'], amount=.4)))\n",
    " .plot.scatter(x='engagements', y='has_emoji', alpha=.1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# :Scatter   [engagements]   (has_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select('time', 'engagements', tweet_col,\n",
    "     reply=tweet_col.str.starts_with('@'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twit\n",
    " .select(pl.col('time').str.to_datetime('%Y-%m-%d %H:%M:%S%z'),\n",
    "         'engagements',\n",
    "         reply=tweet_col.str.starts_with('@'))\n",
    " .pivot(index='time', columns='reply', values='engagements',\n",
    "        aggregate_function='sum')\n",
    " .rename({'false': 'original', 'true': 'reply'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install 'polars[plot]'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(twit\n",
    " .select(pl.col('time').str.to_datetime('%Y-%m-%d %H:%M:%S%z'),\n",
    "         'engagements',\n",
    "         reply=tweet_col.str.starts_with('@'))\n",
    " .pivot(index='time', columns='reply', values='engagements',\n",
    "        aggregate_function='sum')\n",
    " .rename({'false': 'original', 'true': 'reply'})\n",
    " .plot(kind='line', x='time', y=['original', 'reply'])          \n",
    ")\n",
    "\n",
    "# :NdOverlay   [Variable]\n",
    "#    :Curve   [time]   (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(twit\n",
    " .select(pl.col('time').str.to_datetime('%Y-%m-%d %H:%M:%S%z'),\n",
    "         'engagements',\n",
    "         reply=tweet_col.str.starts_with('@'))\n",
    " .pivot(index='time', columns='reply', values='engagements',\n",
    "        aggregate_function='sum')\n",
    " .set_sorted('time')\n",
    " .group_by_dynamic('time', every='1w')\n",
    " .agg(pl.col('true', 'false').mean())\n",
    " .rename({'false': 'original', 'true': 'reply'})\n",
    " .plot(kind='line', x='time', y=['original', 'reply'],\n",
    "      title='Engagements by reply type')\n",
    ")\n",
    "\n",
    "# :NdOverlay   [Variable]\n",
    "#    :Curve   [time]   (value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Functions and Methods\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Aggregation with Polars \n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pl.col('make')\n",
    "print(sorted(att for att in dir(col) if not att.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = pl.col('city08')\n",
    "print(autos\n",
    " .select(mean_city=city.mean(),\n",
    "         std_city=city.std(),\n",
    "         var_city=city.var(),\n",
    "         q99_city=city.quantile(.99)\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".with_columns(mean_city=city.mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = pl.DataFrame({'name':['Tom', 'Sally', 'Jose'],\n",
    "                      'test1':[99, 98, 95],\n",
    "                      'test2':[92, None, 99],\n",
    "                      'test3':[91, 93, 95],\n",
    "                      'test4':[94, 92, 99]})\n",
    "\n",
    "print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "print(tests\n",
    ".select(scores=pl.concat_list(cs.matches(r'test\\d+')))\n",
    ".with_columns(sorted_scores=pl.col('scores').list.sort())\n",
    ".with_columns(slice_scores=pl.col('sorted_scores').list.slice(2,4))\n",
    ".with_columns(sum_scores=pl.col('slice_scores').list.sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests.select(pl.all().exclude('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tests.select(pl.all() - pl.col('name')))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ComputeError: arithmetic on string and numeric not allowed, try an\n",
    "#      explicit cast first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests.select(cs.all() - pl.col('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests.select(cs.starts_with('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests\n",
    ".select(scores=pl.concat_list(cs.matches(r'test\\d+')))\n",
    ".with_columns(max=pl.col('scores').list.max())          \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make')\n",
    " .agg(mean_city=pl.col('city08').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make', maintain_order=True)\n",
    " .agg(mean_city=pl.col('city08').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make')\n",
    " .agg(mean_city=pl.col('city08').mean())\n",
    " .sort('make')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('make').cast(pl.Categorical('lexical')))\n",
    " .group_by('make')\n",
    " .agg(mean_city=pl.col('city08').mean())\n",
    " .sort('make')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (autos\n",
    " .with_columns(pl.col('make').cast(pl.Categorical('lexical')))\n",
    " .group_by('make')\n",
    " .agg(mean_city=pl.col('city08').mean(),\n",
    "      mean_highway=pl.col('highway08').mean(),\n",
    "      median_city=pl.col('city08').median(),\n",
    "      median_highway=pl.col('highway08').median()\n",
    "  )\n",
    " .sort('make')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping By Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('make').cast(pl.Categorical('lexical')))\n",
    " .group_by(['make', 'year'])\n",
    " .agg(mean_city08=pl.col('city08').mean())\n",
    " .sort(['make', 'year'])\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting with Multiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = (autos\n",
    " .group_by('make')\n",
    " .len()\n",
    " .sort(pl.col('len'), descending=True)\n",
    " .head(3)\n",
    " )\n",
    "\n",
    "print(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.col('make').is_in(top3['make']))\n",
    " .group_by(['year', 'make'])\n",
    " .agg(min_city08=pl.col('city08').min(),\n",
    "      max_city08=pl.col('city08').max())\n",
    " .sort(['year', 'make'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.col('make').is_in(top3['make']))\n",
    " .group_by(['year', 'make'])\n",
    " .agg(min_city08=pl.col('city08').min(),\n",
    "      max_city08=pl.col('city08').max())\n",
    " .sort(['year', 'make'])\n",
    " .pivot(index='year', columns='make', \n",
    "        values=['min_city08', 'max_city08'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "(autos\n",
    " .filter(pl.col('make').is_in(top3['make']))\n",
    " .group_by(['year', 'make'])\n",
    " .agg(min_city08=pl.col('city08').min(),\n",
    "      max_city08=pl.col('city08').max())\n",
    " .sort(['year', 'make'])\n",
    " .pivot(index='year', columns='make', \n",
    "        values=['min_city08', 'max_city08'])\n",
    " .select(['year', 'min_city08_make_Dodge', 'max_city08_make_Dodge', \n",
    "          'min_city08_make_Ford', 'max_city08_make_Ford',\n",
    "          'max_city08_make_Chevrolet', 'min_city08_make_Chevrolet'])\n",
    " .to_pandas()\n",
    " .set_index('year')\n",
    " .plot(ax=ax, color=['#55c667', '#2f8738', '#2c4279', '#141f38', \n",
    "                     '#d4e116', '#afba12'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (autos\n",
    " .filter(pl.col('make').is_in(top3['make']))\n",
    " .group_by(['year', 'make'])\n",
    " .agg(min_city08=pl.col('city08').min(),\n",
    "      max_city08=pl.col('city08').max())\n",
    " .sort(['year', 'make'])\n",
    " .pivot(index='year', columns='make', \n",
    "        values=['min_city08', 'max_city08'])\n",
    " .select(['year', 'min_city08_make_Dodge', 'max_city08_make_Dodge', \n",
    "          'min_city08_make_Ford', 'max_city08_make_Ford',\n",
    "          'max_city08_make_Chevrolet', 'min_city08_make_Chevrolet'])\n",
    " .plot(x='year', color=['#55c667', '#2f8738', '#2c4279', '#141f38', \n",
    "                     '#d4e116', '#afba12'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot \n",
    "# saving requires selenium or phantomjs\n",
    "hvplot.save(res, 'img/hvplot1.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating to the Original Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(mean_city08=pl.col('city08')\n",
    "               .mean()\n",
    "               .over('make'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make')\n",
    " .agg(mean_city08=pl.col('city08').mean())\n",
    " .filter(pl.col('make').cast(pl.String)\n",
    "            .is_in(['Alfa Romeo', 'Ferrari', 'Dodge']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping to Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by('make')\n",
    " .agg(pl.all())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select('make', 'model', 'cylinders')                 \n",
    " .group_by('make')\n",
    " .agg(pl.all().unique().drop_nulls().len().name.suffix('_len'))          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('model', 'cylinders')      \n",
    "    .unique().drop_nulls().len()\n",
    "    .over('make')\n",
    "    .name.suffix('_len'))     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select((cs.all() - pl.col('make'))\n",
    "            .unique().drop_nulls().len()\n",
    "            .over('make')\n",
    "            .name.suffix('_len'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Data Filtering and Selection \n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Getting the Data\n",
    "\n",
    "## Filtering with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter(pl.col('city08') > 40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter(pl.col('city08') > 40)\n",
    "  .filter(pl.col('make') == 'Toyota')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter(pl.col('city08') > 40,\n",
    "          pl.col('make') == 'Toyota')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter((pl.col('make') == 'Toyota') & (pl.col('city08') > 40))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter(pl.col('make') == 'Toyota' & pl.col('city08') > 40)\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# TypeError: the truth value of an Expr is ambiguous\n",
    "\n",
    "# You probably got here by using a Python standard library function\n",
    "#      instead of the native expressions API.\n",
    "# Here are some things you might want to try:\n",
    "# - instead of `pl.col('a') and pl.col('b')`, use `pl.col('a') &\n",
    "#      pl.col('b')`\n",
    "# - instead of `pl.col('a') in [y, z]`, use `pl.col('a').is_in([y, z])`\n",
    "# - instead of `max(pl.col('a'), pl.col('b'))`, use\n",
    "#      `pl.max_horizontal(pl.col('a'), pl.col('b'))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(make='Ford')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "mar2018 = datetime(2018, 3, 1)\n",
    "print(autos\n",
    " .filter(pl.col('createdOn') >= mar2018)\n",
    " .sort('createdOn')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.col('city08').is_between(40, 50))\n",
    " .sort('city08')\n",
    " .select(['year', 'make', 'model', 'VClass', 'city08'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "mar2013 = mar2018 - timedelta(days=5*365)\n",
    "print(autos\n",
    ".filter(pl.col('createdOn').is_between(mar2013, mar2018))\n",
    ".sort('createdOn')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Filtering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(col):\n",
    "  return (col - col.mean()) / col.std()\n",
    "\n",
    "print(autos\n",
    "  .filter(standardize(pl.col('city08')) > 3)\n",
    "  .select(['year', 'make', 'model', 'VClass', 'city08'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    "  .filter(standardize(pl.col('city08')).over(['year', 'make']) > 3)\n",
    "  .select(['year', 'make', 'model', 'VClass', 'city08'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Filtering with Window Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(\n",
    "     model_age=(pl.col('year').max() - pl.col('year').min())\n",
    "                .over('model'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(\n",
    "    model_age=(pl.col('year').max() - pl.col('year').min())\n",
    "               .over('model'))\n",
    " .filter(pl.col('model_age') > 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Data with Conditional Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".group_by('fuelType')\n",
    ".len()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.col('fuelType').value_counts(sort=True))\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.col('fuelType').value_counts(sort=True))\n",
    " .unnest('fuelType')          \n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.col('fuelType')\n",
    "         .value_counts(sort=True)\n",
    "         .struct[0])\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Diesel' in fuel_type:\n",
    "    fuel_type_simple = 'Diesel'\n",
    "elif 'CNG' in fuel_type:\n",
    "    fuel_type_simple = 'CNG'\n",
    "elif 'Electricity' in fuel_type:\n",
    "    fuel_type_simple = 'Electric'\n",
    "else:\n",
    "    fuel_type_simple = 'Gasoline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(pl.when(COND_EXPR)\n",
    "   .then(EXPR)\n",
    " .when(COND_EXPR2)\n",
    "   .then(EXPR2)\n",
    " # more when's\n",
    " .otherwise(EXPRN) # optional\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_str = pl.col('fuelType').cast(pl.String)\n",
    "simple = (pl.when(fuel_str.str.contains('Diesel')).then(pl.lit('Diesel'))\n",
    "  .when(fuel_str.str.contains('CNG')).then(pl.lit('CNG'))\n",
    "  .when(fuel_str.str.contains('Electricity')).then(pl.lit('Electric'))\n",
    "  .otherwise(pl.lit('Gasoline')))\n",
    "\n",
    "print(autos\n",
    ".with_columns(simple.alias('fuelTypeSimple'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pl.when(fuel_str.str.contains('Diesel')).then(pl.lit('Diesel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (pl.when(fuel_str.str.contains('Diesel')).then('Diesel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (pl.when(fuel_str.str.contains('Diesel')).then(pl.col('Diesel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_str = pl.col('fuelType').cast(pl.String)\n",
    "simple = (pl.when(fuel_str.str.contains('Diesel')).then(pl.lit('Diesel'))\n",
    "  .when(fuel_str.str.contains('CNG')).then(pl.lit('CNG'))\n",
    "  .when(fuel_str.str.contains('Electricity')).then(pl.lit('Electric'))\n",
    "  .otherwise(pl.lit('Gasoline')))\n",
    "\n",
    "print(autos\n",
    ".with_columns(fuelTypeSimple=simple)\n",
    ".filter(pl.col('fuelTypeSimple') == 'CNG')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_str = pl.col('fuelType').cast(pl.String)\n",
    "simple = (pl.when(fuel_str.str.contains('Diesel')).then(pl.lit('Diesel'))\n",
    "  .when(fuel_str.str.contains('CNG')).then(pl.lit('CNG'))\n",
    "  .when(fuel_str.str.contains('Electricity')).then(pl.lit('Electric'))\n",
    "  .otherwise(pl.lit('Gasoline')))\n",
    "\n",
    "print(autos\n",
    ".filter(simple == 'CNG')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = (pl.col('make')\n",
    "     .value_counts(sort=True)\n",
    "     .struct[0]\n",
    "     .head(10)\n",
    ")\n",
    "\n",
    "top_expr = (pl.when(pl.col('make').is_in(top_n))\n",
    "    .then(pl.col('make'))\n",
    "    .otherwise(pl.lit('Other'))\n",
    ")\n",
    "            \n",
    "print(autos\n",
    " .with_columns(simple_make=top_expr)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit(col_name, n=10, default='other'):\n",
    "    col = pl.col(col_name)    \n",
    "    top_n = (col\n",
    "     .value_counts(sort=True)\n",
    "     .struct[0]\n",
    "     .head(n)\n",
    "    )          \n",
    "    return (pl.when(col.is_in(top_n))\n",
    "     .then(col)\n",
    "     .otherwise(pl.lit(default))\n",
    "    )\n",
    "            \n",
    "print(autos\n",
    " .with_columns(simple_make=limit('make'))\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.StringCache():\n",
    "    autos2 = tweak_auto(raw)\n",
    "    print(autos2\n",
    "     .select(simple_make=limit('make', 20, 'other')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.col('*').is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(True)\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.all().is_null().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.all().null_count())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .null_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.all().is_null().mean() * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .select(pl.all().is_null().cast(pl.Int32).mean() * 100)\n",
    " .pipe(lambda df_: df_.select([col.name \n",
    "           for col in df_.select(pl.col(pl.Float64)> 0) \n",
    "           if col.all()]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "print(autos\n",
    " .select(cs.numeric().is_nan().cast(pl.Int32).mean() * 100)\n",
    " .pipe(lambda df_: df_.select([col.name \n",
    "           for col in df_.select(pl.col(pl.Float64)> 0) \n",
    "           if col.all()]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Rows with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import polars.selectors as cs\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(autos.select(cs.numeric()))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ValueError: Input X contains NaN.\n",
    "# PCA does not accept missing values encoded as NaN natively. For\n",
    "#      supervised learning, you might want to consider\n",
    "#      sklearn.ensemble.HistGradien..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .drop_nulls()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .drop_nulls(subset=(['cylinders', 'displ']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .drop_nulls(subset=cs.numeric())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(autos.select(cs.numeric()).drop_nulls())\n",
    "PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    ".filter(pl.col('num_gears', 'is_automatic').is_null())\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ComputeError: The predicate passed to 'LazyFrame.filter' expanded to\n",
    "#      multiple expressions: \n",
    "\n",
    "# \tcol(\"num_gears\").is_null(),\n",
    "# \tcol(\"is_automatic\").is_null(),\n",
    "# This is ambiguous. Try to combine the predicates with the 'all' or `any'\n",
    "#      expression.\n",
    "\n",
    "# Error originated just after this operation:\n",
    "# DF [\"year\", \"make\", \"model\", \"displ\"]; PROJECT */15 COLUMNS; SELECTION:\n",
    "#      \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".filter(pl.col('num_gears').is_null() & \n",
    "        pl.col('is_automatic').is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(pl.all_horizontal(\n",
    "     pl.col('num_gears', 'is_automatic').is_null()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .filter(~pl.all_horizontal(\n",
    "     pl.col('num_gears', 'is_automatic').is_null()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('cylinders').fill_null(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".select(cyl_mean=pl.col('cylinders').mean(),\n",
    "        cyl_fill0_mean=pl.col('cylinders').fill_null(0).mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Time Series Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "snow = pl.DataFrame({'depth': [0, 0, np.nan, 9.1, 11.3, None, 7.8, \n",
    "                               15, 20]})\n",
    "print(snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snow\n",
    " .with_columns(pl.col('depth'),\n",
    "   depth_ffill=pl.col('depth').fill_null(strategy='forward'),\n",
    "   depth_bfill=pl.col('depth').fill_null(strategy='backward'),\n",
    "   depth_interp=pl.col('depth').interpolate(),\n",
    "   depth_0fill=pl.col('depth').fill_null(0),                                     \n",
    "   depth_mean=pl.col('depth').fill_null(strategy='mean'),\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snow\n",
    " .describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snow\n",
    " .with_columns('depth',\n",
    "   fill_nan=pl.col('depth').fill_nan(None),\n",
    "   depth_ffill=pl.col('depth').fill_null(strategy='forward'),\n",
    "   depth_bfill=pl.col('depth').fill_null(strategy='backward'),\n",
    "   depth_interp=pl.col('depth').interpolate(),\n",
    "   depth_mean=pl.col('depth').fill_null(strategy='mean')\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snow\n",
    " .with_columns('depth',\n",
    "   fill_nan=pl.col('depth').fill_nan(None),\n",
    "   depth_ffill=pl.col('fill_nan').fill_null(strategy='forward'),\n",
    "   depth_bfill=pl.col('fill_nan').fill_null(strategy='backward'),\n",
    "   depth_interp=pl.col('fill_nan').interpolate(),\n",
    "   depth_mean=pl.col('fill_nan').fill_null(strategy='mean')\n",
    " )\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ColumnNotFoundError: fill_nan\n",
    "\n",
    "# Error originated just after this operation:\n",
    "# DF [\"depth\"]; PROJECT */1 COLUMNS; SELECTION: \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pl = (snow\n",
    " .with_columns(fill_nan=pl.col('depth').fill_nan(None))\n",
    " .with_columns(\n",
    "   depth_ffill=pl.col('fill_nan').fill_null(strategy='forward'),\n",
    "   depth_bfill=pl.col('fill_nan').fill_null(strategy='backward'),\n",
    "   depth_interp=pl.col('fill_nan').interpolate(),\n",
    "   depth_mean=pl.col('fill_nan').fill_null(strategy='mean')\n",
    " )\n",
    ")\n",
    "print(imp_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot\n",
    "hvplot.extension('matplotlib')\n",
    "    \n",
    "shift = .8\n",
    "plot=(imp_pl\n",
    " .select(Original=pl.col('depth'),\n",
    "         Forward_fill=pl.col('depth_ffill')+shift,\n",
    "         Backward_fill=pl.col('depth_bfill')+shift * 2,\n",
    "         Interpolate=pl.col('depth_interp')+shift * 3,\n",
    "         Mean_fill=pl.col('depth_mean')+shift * 4,\n",
    "        )\n",
    " .plot(title='Missing Values Demo', width=1000, height=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Sorting and Ordering in Polars\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Loading the Fuel Economy Dataset\n",
    "\n",
    "## Sorting by a Single Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Al', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(names, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('make').cast(pl.String))\n",
    " .sort(by=pl.col('make').str.len_chars())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".sort((pl.col('city08') + pl.col('highway08'))/2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".sort(pl.col('VClass'))\n",
    ")\n",
    "shape: (41_144, 15)\n",
    "│ year   make         model       .   createdOn   is_automa   num_gears │"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".with_columns(make_avg=pl.col('city08').mean().over(pl.col('make')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    ".sort(pl.col('city08').mean().over(pl.col('make')))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .sort(['year', 'make'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .sort(['year', 'make', 'model'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .sort(['year', 'make', 'model'], \n",
    "       descending=[True, False, False])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(pl.col('make').cast(pl.Categorical('lexical')),\n",
    "               pl.col('model').cast(pl.Categorical('lexical')))\n",
    " .sort(['year', 'make', 'model'], \n",
    "       descending=[True, False, False])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Custom Ordering for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .with_columns(month=pl.col('createdOn').dt.strftime('%B'))\n",
    " .sort('month')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', \n",
    "               'December']\n",
    "with pl.StringCache():\n",
    "    pl.Series(month_order).cast(pl.Categorical) \n",
    "    print(autos\n",
    "       .with_columns(month=pl.col('createdOn').dt.strftime('%B')\n",
    "                .cast(pl.Categorical))\n",
    "       .sort('month')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', \n",
    "               'December']\n",
    "@pl.StringCache()\n",
    "def create_month_order():\n",
    "    s = pl.Series(month_order).cast(pl.Categorical) \n",
    "    return (autos\n",
    "       .with_columns(month=pl.col('createdOn').dt.strftime('%B')\n",
    "                    .cast(pl.Categorical))\n",
    "    )\n",
    "\n",
    "print(create_month_order().sort('month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "path = 'data/vehicles.csv'\n",
    "raw = pl.read_csv(path, null_values=['NA'])\n",
    "\n",
    "@pl.StringCache()\n",
    "def tweak_auto(df):\n",
    "  cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany',\n",
    "          'drive', 'VClass', 'fuelType', 'barrels08', 'city08',\n",
    "          'highway08', 'createdOn']\n",
    "  return (df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col('make').cast(pl.Categorical('lexical')),                    \n",
    "       pl.col(['model', 'VClass', 'drive', 'fuelType'])\n",
    "              .cast(pl.Categorical),\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       is_automatic=pl.col('trany').str.contains('Auto'),\n",
    "       num_gears=pl.col('trany').str.extract(r'(\\d+)').cast(pl.Int8)\n",
    "       )\n",
    "   )\n",
    "\n",
    "\n",
    "autos = tweak_auto(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweak_auto(raw)\n",
    " .sort('make'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enums and Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "data = '''Name,Birthday\n",
    "Brianna Smith,2000-02-16\n",
    "Alex Johnson,2001-01-15\n",
    "Carlos Gomez,2002-03-17\n",
    "Diana Clarke,2003-04-18\n",
    "Ethan Hunt,2002-05-19\n",
    "Fiona Gray,2005-06-20\n",
    "George King,2006-07-21\n",
    "Hannah Scott,2007-08-22\n",
    "Ian Miles,2008-09-23\n",
    "Julia Banks,2009-10-24'''\n",
    "\n",
    "students = pl.read_csv(io.StringIO(data))\n",
    "print(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bday = pl.col('Birthday')\n",
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B'))\n",
    "  .sort('month')\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_type = pl.Enum(['January', 'February', 'March', 'April', 'May', \n",
    "   'June', 'July', 'August', 'September', 'October', 'November', \n",
    "   'December'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B').cast(month_type))\n",
    "  .sort('month')\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = '''Month,Holiday\n",
    "January,New Year's Day\n",
    "February,Valentine's Day\n",
    "March,St. Patrick's Day\n",
    "April,April Fools' Day\n",
    "May,Memorial Day\n",
    "June,Juneteenth\n",
    "July,Independence Day\n",
    "August,Labor Day\n",
    "September,Patriot Day\n",
    "October,Halloween\n",
    "November,Thanksgiving\n",
    "December,Christmas Day'''\n",
    "\n",
    "holidays = pl.read_csv(io.StringIO(holiday_data))\n",
    "print(holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B'))\n",
    "  .join(holidays, left_on='month', right_on='Month')\n",
    "  .sort('month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B').cast(pl.Categorical))\n",
    "  .join(holidays, left_on='month', right_on='Month')\n",
    "  .sort('month'))\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# ComputeError: datatypes of join keys don't match - `month`: cat on left\n",
    "#      does not match `Month`: str on right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B').cast(pl.Categorical))\n",
    "  .join(holidays.with_columns(pl.col('Month').cast(pl.Categorical)),\n",
    "     left_on='month', right_on='Month')\n",
    "  .sort('month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "  .with_columns(bday.str.to_datetime('%Y-%m-%d'))\n",
    "  .with_columns(month=bday.dt.strftime('%B').cast(month_type))\n",
    "  .join(holidays.with_columns(pl.col('Month').cast(month_type)),\n",
    "     left_on='month', right_on='Month')\n",
    "  .sort('month'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Ordering and maintain_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by(pl.col('make'))\n",
    " .len()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by(pl.col('make'), maintain_order=True)\n",
    " .len()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .group_by(pl.col('make'))\n",
    " .len()\n",
    " .sort('make')          \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pl.DataFrame({\n",
    "   'name': ['Alice', 'Bob', 'Charlie', 'Dana', 'Eve'],\n",
    "   'age': [25, 20, 25, 21, 24],\n",
    "  'grade': [88, 92, 95, 88, 60],\n",
    "})\n",
    "\n",
    "print(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "    .sort('age')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students\n",
    "    .sort('grade')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(autos.filter(pl.col('year') == 1994))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos_year_sorted = autos.sort('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(autos_year_sorted.filter(pl.col('year') == 1994))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_make = autos\n",
    "string_make = autos.with_columns(make=pl.col('make').cast(pl.String))\n",
    "sorted_make = (autos\n",
    "  .with_columns(make=pl.col('make').cast(pl.String))\n",
    "  .sort('make'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cat_make.filter(pl.col('make') == 'Ford')\n",
    "84.2 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "string_make.filter(pl.col('make') == 'Ford')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sorted_make.filter(pl.col('make') == 'Ford')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Time Series Analysis \n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download_and_modify_url(url, local_filename):\n",
    "    urllib.request.urlretrieve(url, local_filename)\n",
    "    with open(local_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(local_filename, 'w') as file:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i <34 or i == 35:\n",
    "                continue\n",
    "            file.write(line)\n",
    "\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master'\\\n",
    "    '/data/dirtydevil.txt'\n",
    "local_filename = 'data/devilclean.txt'\n",
    "download_and_modify_url(url, local_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def tweak_river(df_):\n",
    "    return (df_\n",
    "        .select('agency_cd', 'site_no', 'tz_cd', \n",
    "                pl.col('datetime').str.to_datetime(),\n",
    "                cfs=pl.col('144166_00060'),\n",
    "                gage_height=pl.col('144167_00065').cast(pl.Float64)\n",
    "                )\n",
    "            )\n",
    "\n",
    "raw = pl.read_csv('data/devilclean.txt', separator='\\t')\n",
    "dd = tweak_river(raw)\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.read_csv('data/devilclean.txt', separator='\\t', \n",
    "                  try_parse_dates=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = '%Y-%m-%d %H:%M'\n",
    "print(raw\n",
    ".select(original=pl.col('datetime'),\n",
    "        to_datetime=pl.col('datetime').str.to_datetime(format),\n",
    "        to_date=pl.col('datetime').str.to_date(format),\n",
    "        strptime=pl.col('datetime').str.strptime(pl.Datetime, format),\n",
    "        # line below fails\n",
    "        #cast=pl.col('datetime').cast(pl.Datetime)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Columns to Create Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw\n",
    " .select(to_datetime=pl.col('datetime').str.to_datetime(format))\n",
    " .with_columns(month=pl.col('to_datetime').dt.strftime('%m'),\n",
    "               year=pl.col('to_datetime').dt.strftime('%Y'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw\n",
    " .select(to_datetime=pl.col('datetime').str.to_datetime(format))\n",
    " .with_columns(month=pl.col('to_datetime').dt.strftime('%m'),\n",
    "               year=pl.col('to_datetime').dt.strftime('%Y'))\n",
    " .select(pl.date(pl.col('year'), pl.col('month'), 1))                   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Timezones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweak_river(raw)\n",
    "['datetime']\n",
    ".dtype.time_zone\n",
    ")\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "pytz.all_timezones[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = '%Y-%m-%d %H:%M'\n",
    "print(raw\n",
    ".select(original=pl.col('datetime'),\n",
    "        naive=pl.col('datetime').str.to_datetime(format),\n",
    "        utc=pl.col('datetime').str.to_datetime(format)\n",
    "            .dt.replace_time_zone('UTC'), \n",
    "        Denver=pl.col('datetime').str.to_datetime(format)\n",
    "            .dt.replace_time_zone('UTC')\n",
    "            .dt.convert_time_zone('America/Denver'),\n",
    "        Denver2=(pl.col('datetime') + ' ' + (pl.col('tz_cd')\n",
    "            .str.replace('MST', '-0700').str.replace('MDT', '-0600')))\n",
    "            .str.to_datetime('%Y-%m-%d %H:%M %z')\n",
    "            .dt.convert_time_zone('America/Denver'),\n",
    "        Denver3=(pl.col('datetime').str.to_datetime(format,\n",
    "                 time_zone='America/Denver', ambiguous='earliest'))\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = '%Y-%m-%d %H:%M'\n",
    "print(raw\n",
    ".select(\n",
    "        Denver2=(pl.col('datetime') + ' ' + (pl.col('tz_cd')\n",
    "            .str.replace('MST', '-0700').str.replace('MDT', '-0600')))\n",
    "            .str.to_datetime('%Y-%m-%d %H:%M %z')\n",
    "            .dt.convert_time_zone('America/Denver'),            \n",
    "        Denver3=(pl.col('datetime').str.to_datetime(format,\n",
    "                 time_zone='America/Denver', ambiguous='latest'))\n",
    "        )\n",
    ".filter(pl.col('Denver3') != pl.col('Denver2'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def tweak_river(df_, cfs_col, gage_height_col):\n",
    "    return (df_\n",
    "        .select(\n",
    "            'agency_cd', 'site_no', \n",
    "            cfs=pl.col(cfs_col),\n",
    "            gage_height=pl.col(gage_height_col).cast(pl.Float64),\n",
    "            datetime=(pl.col('datetime') + ' ' + (pl.col('tz_cd')\n",
    "                .str.replace('MST', '-0700').str.replace('MDT', '-0600')))\n",
    "                .str.to_datetime('%Y-%m-%d %H:%M %z')\n",
    "                .dt.convert_time_zone('America/Denver')\n",
    "            )\n",
    "        )\n",
    "\n",
    "dd = tweak_river(raw, cfs_col='144166_00060', \n",
    "                gage_height_col='144167_00065')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .with_columns(year=pl.col('datetime').dt.year())\n",
    " .group_by('year')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .with_columns(year=pl.col('datetime').dt.year())\n",
    " .group_by('year', maintain_order=True)\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dd\n",
    " .group_by_dynamic(index_column='datetime', every='1y')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    " )\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# InvalidOperationError: argument in operation 'group_by_dynamic' is not\n",
    "#      explicitly sorted\n",
    "\n",
    "# - If your data is ALREADY sorted, set the sorted flag with:\n",
    "#      '.set_sorted()'.\n",
    "# - If your data is NOT sorted, sort the 'expr/series/column' first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .sort('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='1y')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='1y')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='2mo')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='3h4m5s')\n",
    " .agg(pl.col(pl.Float64).mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='7d', period='5d', \n",
    "                   start_by='monday', check_sorted=False)\n",
    " .agg(pl.col(pl.Float64).mean(),\n",
    "     cfs_range=(pl.col('cfs').max() - pl.col('cfs').min()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='7d', period='2d', \n",
    "                   start_by='saturday', check_sorted=False)\n",
    " .agg(pl.col(pl.Float64).mean(),\n",
    "     cfs_range=(pl.col('cfs').max() - pl.col('cfs').min()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .with_columns(year=pl.col('datetime').dt.year())\n",
    " .with_columns(year_mean_cfs=pl.col('cfs').mean().over('year'))\n",
    " .with_columns(pct_of_avg=(pl.col('cfs') / pl.col('year_mean_cfs'))\n",
    "                  .mul(100).round(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .with_columns(year=pl.col('datetime').dt.year(),\n",
    "               quarter=pl.col('datetime').dt.quarter())\n",
    " .with_columns(pl.col(['cfs', 'gage_height']).mean().over('year')\n",
    "                .name.suffix('_mean_year'),\n",
    "               pl.col(['cfs', 'gage_height']).mean().over('quarter')\n",
    "                .name.suffix('_mean_quarter'))                   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Groupings with Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake_raw = (pl.read_csv('data/nwis.waterservices.usgs.gov.txt', \n",
    "                     skip_rows=27, separator='\\t',\n",
    "                     skip_rows_after_header=1))\n",
    "snake = (tweak_river(snake_raw\n",
    "           .with_columns(cfs=pl.lit(None).cast(pl.Float64)),\n",
    "      cfs_col='cfs', gage_height_col='319803_00065') \n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = pl.DataFrame({'b': [1,2,3],\n",
    "             'c': [4,5,6],\n",
    "             'a': [7,8,9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abc.with_columns(['a', 'b', 'c']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abc.select(['a', 'b', 'c']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = ['agency_cd', 'site_no', 'datetime', \n",
    "               'cfs', 'gage_height', 'source']\n",
    "print(snake\n",
    " .with_columns(source=pl.lit('snake'))\n",
    " .select(common_cols)\n",
    " .vstack(dd\n",
    "         .with_columns(source=pl.lit('devil'))\n",
    "         .select(common_cols)\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = ['agency_cd', 'site_no', 'datetime', \n",
    "               'cfs', 'gage_height', 'source']\n",
    "print(snake\n",
    " .with_columns(source=pl.lit('snake'))\n",
    " .select(common_cols)\n",
    " .vstack(dd\n",
    "         .with_columns(source=pl.lit('devil'))\n",
    "         .select(common_cols)\n",
    "         )\n",
    " .sort('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='1mo', \n",
    "                   by='source')\n",
    " .agg(pl.col('gage_height').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = ['agency_cd', 'site_no', 'datetime', \n",
    "               'cfs', 'gage_height', 'source']\n",
    "print(snake\n",
    " .with_columns(source=pl.lit('snake'))\n",
    " .select(common_cols)\n",
    " .vstack(dd\n",
    "         .with_columns(source=pl.lit('devil'))\n",
    "         .select(common_cols)\n",
    "         )\n",
    " .sort('datetime')\n",
    " .group_by([pl.col('datetime').dt.truncate('1mo'), 'source'])\n",
    " .agg(pl.col('gage_height').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = ['agency_cd', 'site_no', 'datetime', \n",
    "               'cfs', 'gage_height', 'source']\n",
    "print(snake\n",
    " .with_columns(source=pl.lit('snake'))\n",
    " .select(common_cols)\n",
    " .vstack(dd\n",
    "         .with_columns(source=pl.lit('devil'))\n",
    "         .select(common_cols)\n",
    "         )\n",
    " .sort('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='1mo', by='source')\n",
    " .agg(pl.col('gage_height').mean())\n",
    " .pivot(columns='source', index='datetime', values='gage_height')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot\n",
    "hvplot.extension('matplotlib')\n",
    "common_cols = ['agency_cd', 'site_no', 'datetime', \n",
    "               'cfs', 'gage_height', 'source']\n",
    "print(snake\n",
    " .with_columns(source=pl.lit('snake'))\n",
    " .select(common_cols)\n",
    " .vstack(dd\n",
    "         .with_columns(source=pl.lit('devil'))\n",
    "         .select(common_cols)\n",
    "         )\n",
    " .sort('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='1mo', by='source')\n",
    " .agg(pl.col('gage_height').mean())\n",
    " .pivot(columns='source', index='datetime', values='gage_height')\n",
    " .filter(~pl.all_horizontal(pl.col('devil', 'snake').is_null()))\n",
    " .plot(x='datetime', y=['devil', 'snake'], rot=45, title='Gage Height', \n",
    "       width=1800, height=600)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Window Functions in Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='1mo')\n",
    " .agg(pl.col('cfs').mean())\n",
    " .with_columns(mean_cfs_3mo=pl.col('cfs').mean()\n",
    "               .rolling('datetime', period='3mo'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot\n",
    "hvplot.extension('plotly')\n",
    "(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='1mo')\n",
    " .agg(pl.col('cfs').mean())\n",
    " .with_columns(mean_cfs_3mo=pl.col('cfs').mean()\n",
    "               .rolling('datetime', period='3mo'))\n",
    " .plot(x='datetime', y=['cfs', 'mean_cfs_3mo'])\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs_max = 1_000\n",
    "(dd\n",
    " .set_sorted('datetime')  \n",
    " .group_by_dynamic(index_column='datetime', every='1mo')\n",
    " .agg(pl.col('cfs').mean())\n",
    " .with_columns(pl.col('cfs').clip(upper_bound=cfs_max),\n",
    "               mean_cfs_3mo=pl.col('cfs').mean()\n",
    "               .rolling('datetime', period='3mo')\n",
    "               .clip(upper_bound=cfs_max))                   \n",
    " .plot(x='datetime', y=['cfs', 'mean_cfs_3mo'])\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    ".filter(pl.col('cfs').is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "denver = pytz.timezone('America/Denver')\n",
    "jul_7 = datetime.datetime(2018, 7, 7).replace(tzinfo=denver)\n",
    "jul_9 = datetime.datetime(2018, 7, 9).replace(tzinfo=denver)\n",
    "\n",
    "print(dd\n",
    " .filter(pl.col('datetime').is_between(jul_7, jul_9))\n",
    " .filter(pl.col('cfs').is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dd\n",
    " .filter(pl.col('datetime').is_between(jul_7, jul_9))\n",
    "  .plot(x='datetime', y=['cfs'])\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = .1\n",
    "\n",
    "(dd\n",
    " .filter(pl.col('datetime').is_between(jul_7, jul_9))\n",
    " .with_columns(\n",
    "    'datetime', 'cfs',\n",
    "    fill0=pl.col('cfs').fill_null(0).add(offset),\n",
    "    interpolate=pl.col('cfs').interpolate().add(offset*2),\n",
    "    forward=pl.col('cfs').fill_null(strategy='forward').add(offset*3),\n",
    "    backward=pl.col('cfs').fill_null(strategy='backward').add(offset*4)\n",
    " )\n",
    "  .plot(x='datetime', y=['cfs', 'fill0', 'interpolate', 'forward', \n",
    "                         'backward'])\n",
    " ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')\n",
    " .upsample('datetime', every='5m')\n",
    " .interpolate()\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='5m')\n",
    " .agg(pl.col('cfs').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd\n",
    " .set_sorted('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='30m')\n",
    " .agg(pl.col('cfs').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/'\\\n",
    "      'hanksville.csv'\n",
    "\n",
    "def tweak_temp(df_):\n",
    "    return (df_\n",
    "        .select(pl.col('DATE').str.to_datetime()\n",
    "                  .dt.replace_time_zone('America/Denver'),\n",
    "                'PRCP', 'TMIN', 'TMAX', 'TOBS')\n",
    "    )\n",
    "\n",
    "raw_temp = pl.read_csv(url)\n",
    "print(tweak_temp(raw_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_daily = (dd\n",
    " .set_sorted('datetime')\n",
    " .group_by_dynamic(index_column='datetime', every='1d', \n",
    "                   check_sorted=False)\n",
    " .agg(pl.col('gage_height', 'cfs').mean())\n",
    ")\n",
    "\n",
    "print(dd_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = (tweak_temp(raw_temp)\n",
    " .join(dd_daily, left_on='DATE', right_on='datetime',\n",
    "       validate='1:1')\n",
    " )\n",
    "\n",
    "print(both)\n",
    "                                                              │"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hvplot.extension('bokeh')\n",
    "year_agg = (both\n",
    " .with_columns(day_of_year=pl.col('DATE')\n",
    "                     .dt.strftime('%j').cast(pl.Int16),\n",
    "               year=pl.col('DATE').dt.year())\n",
    " .pivot(index='day_of_year', columns='year', values='TOBS')\n",
    " .sort('day_of_year')\n",
    ")\n",
    "\n",
    "p1 = (year_agg\n",
    " .with_columns(\n",
    "     pl.col(['2001', '2002', '2003', '2004', '2005', '2006', \n",
    "             '2007', '2008', '2009', '2010', '2011', '2012', \n",
    "             '2014', '2015', '2016', '2017', '2018', '2019',# '2020'\n",
    "            ])\n",
    "    .rolling_median(7)\n",
    "  )\n",
    " .plot(x='day_of_year',  alpha=.5, line_width=1, \n",
    "       color=hv.Palette('Greys'), \n",
    "       title='Weekly Temperature (F)', width=1_000, height=500)\n",
    " )\n",
    "\n",
    "# make 2020 thicker and blue\n",
    "p2 = p1 * (year_agg\n",
    " .select(pl.col('day_of_year', '2020'))\n",
    "    .plot(x='day_of_year', y='2020', color='blue', line_width=2, \n",
    "          label='2020')\n",
    ")\n",
    "\n",
    "# add median in red\n",
    "p2 * (both\n",
    "  .with_columns(day_of_year=pl.col('DATE')\n",
    "                      .dt.strftime('%j').cast(pl.Int16),\n",
    "                median=pl.lit('Median'))\n",
    " .pivot(index='day_of_year', columns='median', values='TOBS', \n",
    "        aggregate_function='median')\n",
    " .sort('day_of_year')\n",
    "    .plot(x='day_of_year', y='Median', c='r', label='Median')\n",
    " )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year_last_year_median(df, col, upper_limit=None, \n",
    "                               lower_limit=None, width=1_000, \n",
    "                               height=500):\n",
    "      if upper_limit is None:\n",
    "            upper_limit = df[col].max()\n",
    "\n",
    "      year_agg = (df\n",
    "      .with_columns(day_of_year=pl.col('DATE')\n",
    "                          .dt.strftime('%j').cast(pl.Int16),\n",
    "                    year=pl.col('DATE').dt.year())\n",
    "      .pivot(index='day_of_year', columns='year', values=col)\n",
    "      .sort('day_of_year')\n",
    "      )\n",
    "   \n",
    "      # previous years in grey thin lines\n",
    "      p1 = (year_agg\n",
    "      .with_columns(pl.col(\n",
    "          ['2001', '2002', '2003', '2004', '2005', '2006', \n",
    "           '2007', '2008', '2009', '2010', '2011', '2012', \n",
    "           '2014', '2015', '2016', '2017', '2018', '2019',# '2020'\n",
    "          ])\n",
    "         .rolling_median(7).clip(upper_bound=upper_limit))\n",
    "      .plot(x='day_of_year', alpha=1, color=hv.Palette('Greys'), \n",
    "            line_width=.5, width=width, height=height)\n",
    "      )\n",
    "\n",
    "      # make 2020 thicker and blue\n",
    "      p2 = p1*(year_agg\n",
    "      .select('day_of_year', pl.col('2020')\n",
    "                     .rolling_median(7).clip(upper_bound=upper_limit))      \n",
    "         .plot(x='day_of_year', y='2020', color='blue', line_width=2, \n",
    "               title=f'Weekly {col}', label='2020')\n",
    "      )\n",
    "   \n",
    "      # add median in red\n",
    "      p3 = p2*(df\n",
    "         .with_columns(day_of_year=pl.col('DATE')\n",
    "                         .dt.strftime('%j').cast(pl.Int16),\n",
    "                       median=pl.lit('Median'))\n",
    "      .pivot(index='day_of_year', columns='median', values=col, \n",
    "             aggregate_function='median')\n",
    "      .sort('day_of_year')\n",
    "      .with_columns(pl.col('Median')\n",
    "                     .rolling_mean(7).clip(upper_bound=upper_limit))      \n",
    "         .plot(x='day_of_year', y='Median', color='red', label='Median')\n",
    "      )\n",
    "      return p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_last_year_median(both, 'cfs', upper_limit=200)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Data Import and Export\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "path = 'data/vehicles.csv'\n",
    "raw = pl.read_csv(path, null_values=['NA'])\n",
    "\n",
    "def tweak_auto(df):\n",
    "  cols = ['year', 'make', 'model', 'displ', 'cylinders', 'trany', \n",
    "          'drive','VClass', 'fuelType', 'barrels08', 'city08', \n",
    "          'highway08', 'createdOn']\n",
    "  return (df\n",
    "  .select(pl.col(cols))\n",
    "  .with_columns(pl.col('year').cast(pl.Int16),\n",
    "       pl.col(['cylinders', 'highway08', 'city08']).cast(pl.UInt8),\n",
    "       pl.col(['displ', 'barrels08']).cast(pl.Float32),\n",
    "       pl.col(['make', 'model', 'VClass', 'drive', 'fuelType'])\n",
    "         .cast(pl.Categorical),\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       is_automatic=pl.col('trany').str.contains('Auto'),\n",
    "       num_gears=pl.col('trany').str.extract(r'(\\d+)').cast(pl.Int8)\n",
    "       )\n",
    "   )\n",
    "\n",
    "\n",
    "autos = tweak_auto(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .head(3)          \n",
    " .write_csv()\n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autos\n",
    " .head(3)          \n",
    " .write_csv(float_precision=2)\n",
    ")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "pprint.pprint(json.loads(\n",
    "   autos.head(2)\n",
    "       .write_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "print(pl.read_json(StringIO(autos.write_json())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.head(3).to_pandas().to_json('/tmp/pd.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "pprint.pprint(json.loads(autos.head(2)\n",
    "       .to_pandas().to_json(orient='records')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pd = pl.read_json('/tmp/pd.json')\n",
    "print(from_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_pd\n",
    " .with_columns(createdOn=pl.from_epoch('createdOn', time_unit='s'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSON Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "pprint.pprint(json.loads(autos.head(2)\n",
    "       .to_pandas().to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "print(pl.read_json(io.StringIO(autos.head(2).to_pandas().to_json())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame({\n",
    "    'num': [1, 2, 3], \n",
    "    'listy': [[1, 2], [3, 4], [5, 6]],\n",
    "    'structy': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, \n",
    "                {'a': 5, 'b': 6}],\n",
    "    }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame({\n",
    "    'num': [1, 2, 3], \n",
    "    'listy': [[1, 2], [3, 4], [5, 6]],\n",
    "    'structy': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}],\n",
    "    }\n",
    "    )\n",
    "   .explode('listy')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame({\n",
    "    'num': [1, 2, 3], \n",
    "    'listy': [[1, 2], [3, 4], [5, 6]],\n",
    "    'structy': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}],\n",
    "    }\n",
    "    )\n",
    "   .explode('structy')\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# InvalidOperationError: `explode` operation not supported for dtype\n",
    "#      `struct[2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame({\n",
    "    'num': [1, 2, 3], \n",
    "    'listy': [[1, 2], [3, 4], [5, 6]],\n",
    "    'structy': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]})\n",
    "   .with_columns(structy=pl.col('structy').map_elements(\n",
    "       lambda d: list(d.values())\n",
    "       )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "print(pl.read_json(io.StringIO(autos.to_pandas().to_json()))\n",
    " .with_columns(pl.all().map_elements(lambda d: list(d.values())))\n",
    " .explode(pl.all())\n",
    " .with_columns(createdOn=pl.from_epoch('createdOn', time_unit='s'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munging JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json.loads(autos.head(2).to_pandas()\n",
    "                          .to_json(orient='split')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.read_json(io.StringIO(autos.to_pandas()\n",
    "                                .to_json(orient='split'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_json_to_dict(json_str):\n",
    "    \"\"\" Convert pandas \"split\" json to a sequence of dictionaries\n",
    "    representing the rows of the dataframe.\n",
    "    \"\"\"\n",
    "    data = json.loads(json_str)\n",
    "    columns = data['columns']\n",
    "    for row in data['data']:\n",
    "        yield dict(zip(columns, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame(\n",
    "   split_json_to_dict(autos.to_pandas().to_json(orient='split'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .head(3)\n",
    " .write_excel('/tmp/autos.xlsx')\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = (pl.read_excel('/tmp/autos.xlsx'))\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    ".head(3)\n",
    ".write_parquet('/tmp/a3.parquet')\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_parquet('/tmp/a3.parquet').to_parquet('/tmp/a4.parquet')\n",
    "\n",
    "a4 = pl.read_parquet('/tmp/a4.parquet')\n",
    "print(a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.head(3).equals(a4)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    ".head(3)\n",
    ".select(pl.all().shrink_dtype())     \n",
    ".write_parquet('/tmp/a3-shrink.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "with sqlite3.connect('/tmp/vehicles.db') as conn:\n",
    "    uri = 'sqlite:////tmp/vehicles.db'\n",
    "    autos.head(3).write_database(table_name='autos', connection=uri, \n",
    "                                 if_table_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "uri = 'sqlite:////tmp/vehicles.db'\n",
    "with create_engine(uri).connect() as conn:\n",
    "    query = 'SELECT * FROM autos'\n",
    "    a4 = pl.read_database(query=query, connection=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Arrow to Convert DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "sql = '''SELECT mean(city08) AS mean_city08,\n",
    "      mean(highway08) AS mean_highway08,\n",
    "      year\n",
    "FROM autos\n",
    "GROUP BY year'''\n",
    "\n",
    "agg = duckdb.sql(sql)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_agg = agg.pl().to_pandas()\n",
    "print(pd_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "(agg\n",
    " .pl()\n",
    " .to_pandas()\n",
    " .set_index('year')\n",
    " .plot(ax=ax, title='Average MPG by Year'))\n",
    "fig.savefig('img/agg1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "autos = tweak_auto(raw)\n",
    "sql = '''SELECT mean(city08) AS mean_city08,\n",
    "      mean(highway08) AS mean_highway08,\n",
    "      year\n",
    "FROM autos\n",
    "GROUP BY year\n",
    "ORDER BY year'''\n",
    "agg = duckdb.sql(sql)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "(agg\n",
    " .pl()\n",
    " .to_pandas()\n",
    " .set_index('year')\n",
    " .plot(ax=ax, title='Average MPG by Year'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos2 = (autos.to_pandas(use_pyarrow_extension_array=True)\n",
    " .pipe(pl.from_pandas)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos2.equals(autos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Other Libraries\n",
    "\n",
    "## Using XGBoost to Predict Mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "X = (autos\n",
    " .select(cs.numeric() - cs.matches('(city08|highway08)'))\n",
    ")\n",
    "y = (autos.select(pl.col('city08')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xg = xgb.XGBRegressor()\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Residuals with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test.to_series() - xg.predict(X_test) \n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(y_test, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(pl.DataFrame(xg.predict(X_test)) , residuals, alpha=0.1)\n",
    "ax.set_title('City MPG Residuals vs. Predicted City MPG')\n",
    "ax.set_xlabel('Predicted City MPG')\n",
    "ax.set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA of the Autos Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, decomposition\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='polars')\n",
    "std = preprocessing.StandardScaler()\n",
    "X_std = std.fit_transform(\n",
    "   autos.select(pl.col(['displ', 'cylinders', 'barrels08', 'city08', \n",
    "                        'highway08'])\n",
    "      .fill_null(0)))\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "res = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_naming(name):\n",
    "    return f'{name.upper()[:2]}{int(name[-1])+1}'   \n",
    "(res\n",
    ".rename(fix_naming)\n",
    ".with_columns(color=autos['cylinders'])\n",
    ".plot.scatter('PC1', 'PC2', color='color',\n",
    "    title='PCA of Autos', cmap='viridis')\n",
    ")\n",
    "\n",
    "# :Scatter   [PC1]   (PC2,color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "pl.Config.set_tbl_width_chars(70)\n",
    "pl.Config.set_float_precision(2)\n",
    "pl.Config.set_tbl_cols(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(set_tbl_width_chars=70):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Being Lazy and Streaming\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     pl.when(pl.col('make') == 'Chevrolet')\n",
    "     .then('USA')\n",
    "     .when(pl.col('make') == 'Ford')\n",
    "     .then('USA')\n",
    "        ...\n",
    "     .when(pl.col('make') == 'Tesla')\n",
    "     .then('USA')\n",
    "     .otherwise('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def make_to_origin_expr(make_col):\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    expr = None\n",
    "    col = pl.col(make_col)\n",
    "    for k, v in origin_dict.items():\n",
    "        if expr is None:\n",
    "            expr = pl.when(col == k).then(pl.lit(v))\n",
    "        else:\n",
    "            expr = expr.when(col == k).then(pl.lit(v))\n",
    "    expr = expr.otherwise(pl.lit('Unknown'))\n",
    "    return expr\n",
    "\n",
    "df_pl = pl.read_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "result = (df_pl\n",
    "      .with_columns(\n",
    "            pl.col('createdOn')\n",
    "                   .str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "            origin=make_to_origin_expr('make'))\n",
    "      .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "      .select(['make', 'model', 'year', 'city08', 'highway08', \n",
    "               'origin', 'createdOn'])\n",
    "      .group_by(['origin', 'year'])\n",
    "      .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "      .pivot(index='year', columns='origin', values='avg_city08')\n",
    "      .sort('year')\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pl = pl.read_csv('data/vehicles.csv', null_values='NA')\n",
    "result = (df_pl\n",
    "     .with_columns(\n",
    "            pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "            origin=make_to_origin_expr('make'))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "              'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The replace Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_origin_replace(make_col):\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }   \n",
    "    return make_col.replace(origin_dict, default='Unknown')\n",
    "\n",
    "result = (df_pl\n",
    "    .with_columns(\n",
    "       pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "       origin=make_to_origin_replace(pl.col('make')))\n",
    "    .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "    .select(['make', 'model', 'year', 'city08', 'highway08', 'origin',\n",
    "             'createdOn'])\n",
    "    .group_by(['origin', 'year'])\n",
    "    .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "    .pivot(index='year', columns='origin', values='avg_city08')\n",
    "    .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pl = pl.read_csv('data/vehicles.csv', null_values='NA')\n",
    "result = (df_pl\n",
    "    .with_columns(\n",
    "            pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "            origin=make_to_origin_replace(pl.col('make')))\n",
    "    .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "    .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "             'createdOn'])\n",
    "    .group_by(['origin', 'year'])\n",
    "    .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "    .pivot(index='year', columns='origin', values='avg_city08')\n",
    "    .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Version Take One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "result = (df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")\n",
    "\n",
    "print(result)\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# AttributeError: 'LazyFrame' object has no attribute 'pivot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "result = (df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .collect()\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "result = (df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .collect()\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "result = (df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     #.collect()\n",
    "     .fetch(5)\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "import pandas as pd\n",
    "def make_to_origin(make):\n",
    "    \"\"\"\n",
    "    Convert car make to country of origin.\n",
    "    \n",
    "    Args:\n",
    "        make (str): Car make.\n",
    "        \n",
    "    Returns:\n",
    "        str: Country of origin.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make, \"Unknown\")\n",
    "\n",
    "df_pd = pd.read_csv('data/vehicles.csv', \n",
    "                 engine='pyarrow', dtype_backend='pyarrow')\n",
    "\n",
    "(df_pd\n",
    " .assign(origin=lambda df: df['make'].apply(make_to_origin),\n",
    "         # replace EST and EDT with offset in createdOn\n",
    "        createdOn=lambda df: df['createdOn']\n",
    "           .str.replace('EDT', '-04:00').str.replace('EST', '-05:00')\n",
    " )\n",
    " .assign(\n",
    "        createdOn=lambda df: pd.to_datetime(df['createdOn'], \n",
    "            format='%a %b %d %H:%M:%S %z %Y', utc=True),\n",
    " )\n",
    " .query('origin != \"Unknown\" and year < 2020')\n",
    " .loc[:, ['make', 'model', 'year', 'city08', 'highway08', 'origin',\n",
    "          'createdOn']]\n",
    "    .groupby(['origin', 'year'])\n",
    "    .city08\n",
    "    .mean()\n",
    "    .unstack('origin')\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "print(df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     #.collect()\n",
    ")\n",
    "# naive plan: (run LazyFrame.explain(optimized=True) to see the optimized\n",
    "#      plan)\n",
    "\n",
    "# AGGREGATE\n",
    "# \t[col(\"city08\").mean().alias(\"avg_city08\")] BY [col(\"origin\"),\n",
    "#      col(\"year\")] FROM\n",
    "#    SELECT [col(\"make\"), col(\"model\"), col(\"year\"), col(\"city08\"),\n",
    "#      col(\"highway08\"), col(\"origin\"), col(\"createdOn\")] FROM\n",
    "#     FILTER [([(col(\"origin\")) != (String(Unknown))]) & ([(col(\"year\")) <\n",
    "#      (2020)])] FROM\n",
    "\n",
    "#      WITH_COLUMNS:\n",
    "#      [col(\"createdOn\").str.strptime([String(raise)]),\n",
    "#      col(\"make\").replace([Series, Series, String(Unknown)]).alias(\"origin\")]\n",
    "\n",
    "#         Csv SCAN data/vehicles.csv\n",
    "#         PROJECT */83 COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_lazy = pl.scan_csv('data/vehicles.csv', null_values='NA')\n",
    "\n",
    "print(df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .explain()\n",
    "     #.collect()\n",
    ")\n",
    "# AGGREGATE\n",
    "# \t[col(\"city08\").mean().alias(\"avg_city08\")] BY [col(\"origin\"),\n",
    "#      col(\"year\")] FROM\n",
    "#   FAST_PROJECT: [year, city08, origin]\n",
    "#     FILTER [([(col(\"origin\")) != (String(Unknown))]) & ([(col(\"year\")) <\n",
    "#      (2020)])] FROM\n",
    "\n",
    "#      WITH_COLUMNS:\n",
    "#      [col(\"make\").replace([Series, Series,\n",
    "#      String(Unknown)]).alias(\"origin\")]\n",
    "\n",
    "#         Csv SCAN data/vehicles.csv\n",
    "#         PROJECT 3/83 COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "         'createdOn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 400\n",
    "df_pl_lazy = pl.scan_csv(['data/vehicles.csv']*factor, null_values='NA')\n",
    "print(df_pl_lazy\n",
    "     .with_columns(\n",
    "         pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'),\n",
    "         origin=make_to_origin_replace(pl.col('make')))\n",
    "     .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "     .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', \n",
    "                 'createdOn'])\n",
    "     .group_by(['origin', 'year'])\n",
    "     .agg(avg_city08=pl.col(\"city08\").mean())\n",
    "     .collect(streaming=True)\n",
    "     .pivot(index='year', columns='origin', values='avg_city08')\n",
    "     .sort('year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = make_to_origin_expr('make')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = pl.col('make').map_elements(make_to_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods and Functions\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Porting from Pandas\n",
    "\n",
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 13: Portfolio Management System\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "K = 1000000\n",
    "lot = 100\n",
    "port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                'UPS','F']\n",
    "bm_ticker = '^GSPC'\n",
    "ticker_list = [bm_ticker] + port_tickers\n",
    "df_data= { \n",
    "'Beta':[1.34,2,0.75,1.2,0.41,0.95,1.23,0.9,1.05,1.15],\n",
    "'Shares':[-1900,-100,-400,-800,-5500,1600,1800,2800,1100,20800],\n",
    "'rSL':[42.75,231,156,54.2,37.5,42.75,29.97,59.97,39.97,2.10]\n",
    "}\n",
    "port = pd.DataFrame(df_data,index=port_tickers)\n",
    "port['Side'] = np.sign(port['Shares'])\n",
    "\n",
    "raw_data = pd.read_csv('data/raw-yfinance.csv', index_col=0, header=[0,1])\n",
    "price_df = round(raw_data['Close'], 2)\n",
    "\n",
    "bm_cost = price_df[bm_ticker][0]\n",
    "bm_price = price_df[bm_ticker][-1]\n",
    "\n",
    "port['rCost'] = round(price_df.iloc[0,:].div(bm_cost) *1000, 2)\n",
    "port['rPrice'] = round(price_df.iloc[-1,:].div(bm_price) *1000, 2)\n",
    "port['Cost'] = price_df.iloc[0,:]\n",
    "port['Price'] = price_df.iloc[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df['bm returns'] = round(np.exp(np.log(price_df[bm_ticker]/\n",
    "                        price_df[bm_ticker].shift()).cumsum()) - 1, 3)\n",
    "rel_price = round(price_df.div(price_df['^GSPC'],axis=0 )*1000,2)\n",
    "\n",
    "rMV = rel_price.mul(port['Shares'])\n",
    "rLong_MV = rMV[rMV >0].sum(axis=1)\n",
    "rShort_MV = rMV[rMV <0].sum(axis=1)\n",
    "rMV_Beta = rMV.mul(port['Beta'])\n",
    "rLong_MV_Beta = rMV_Beta[rMV_Beta >0].sum(axis=1) / rLong_MV\n",
    "rShort_MV_Beta = rMV_Beta[rMV_Beta <0].sum(axis=1)/ rShort_MV\n",
    "\n",
    "price_df['rNet_Beta'] = rLong_MV_Beta - rShort_MV_Beta\n",
    "price_df['rNet'] = round((rLong_MV + rShort_MV)\n",
    "                         .div(abs(rMV).sum(axis=1)),3)\n",
    "\n",
    "price_df['rReturns_Long'] = round(\n",
    "    np.exp(np.log(rLong_MV/rLong_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns_Short'] = - round(\n",
    "    np.exp(np.log(rShort_MV/rShort_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns'] = price_df['rReturns_Long'] + \\\n",
    "    price_df['rReturns_Short']\n",
    "\n",
    "MV = price_df.mul(port['Shares'])\n",
    "Long_MV = MV[MV >0].sum(axis=1)\n",
    "Short_MV = MV[MV <0].sum(axis=1)\n",
    "price_df['Gross'] = round((Long_MV - Short_MV).div(K),3)\n",
    "price_df['Net'] = round((Long_MV + Short_MV).div(abs(MV).sum(axis=1)),3)\n",
    "\n",
    "price_df['Returns_Long'] = round(\n",
    "    np.exp(np.log(Long_MV/Long_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns_Short'] = -round(\n",
    "    np.exp(np.log(Short_MV/Short_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns'] = price_df['Returns_Long'] + price_df['Returns_Short']\n",
    "\n",
    "MV_Beta = MV.mul(port['Beta'])\n",
    "Long_MV_Beta = MV_Beta[MV_Beta >0].sum(axis=1) / Long_MV\n",
    "Short_MV_Beta = MV_Beta[MV_Beta <0].sum(axis=1)/ Short_MV\n",
    "price_df['Net_Beta'] = Long_MV_Beta - Short_MV_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplication Diversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "days = pd.Series([10, 11, 9], index=['Suzy', 'Bob', 'Alice'])\n",
    "daily_consumption = pd.DataFrame({'Suzy': [.2, .9, .1],\n",
    "                                  'Bob': [.3, .2, .8],\n",
    "                                  'Alice': [.5, .1, .1],\n",
    "                                  'Joe': [.1, .8, .0]},\n",
    " index=['meat', 'plants', 'dairy'])\n",
    "\n",
    "print(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_consumption * days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "days_pl = pl.DataFrame(days.reset_index())\n",
    "print(days_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_consumption_pl = pl.DataFrame(daily_consumption.reset_index())\n",
    "print(daily_consumption_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_consumption_pl * days_pl)\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# PanicException: data types don't match: InvalidOperation(ErrString(\"mul\n",
    "#      operation not supported for dtypes `str` and `str`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name,i, val) for i, (name, val) in enumerate(days_pl.iter_rows())]\n",
    "# [('Suzy', 0, 10), ('Bob', 1, 11), ('Alice', 2, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_consumption_pl\n",
    " .with_columns([pl.col(name) * val \n",
    "           for i, (name, val) in enumerate(days_pl.iter_rows())])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_dict = days.to_dict()\n",
    "days_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_consumption_pl\n",
    " .with_columns(pl.col(name) * val \n",
    "           for name, val in days_dict.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging AI\n",
    "\n",
    "## Code Review\n",
    "\n",
    "## Enhancing Your Coding Process\n",
    "\n",
    "## Analyzing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1000000\n",
    "lot = 100\n",
    "port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                'UPS','F']\n",
    "bm_ticker = '^GSPC'\n",
    "ticker_list = [bm_ticker] + port_tickers\n",
    "df_data= { \n",
    "'Beta':[1.34,2,0.75,1.2,0.41,0.95,1.23,0.9,1.05,1.15],\n",
    "'Shares':[-1900,-100,-400,-800,-5500,1600,1800,2800,1100,20800],\n",
    "'rSL':[42.75,231,156,54.2,37.5,42.75,29.97,59.97,39.97,2.10]\n",
    "}\n",
    "port = pd.DataFrame(df_data,index=port_tickers)\n",
    "port['Side'] = np.sign(port['Shares'])\n",
    "\n",
    "raw_data = pd.read_csv('data/raw-yfinance.csv', index_col=0, header=[0,1])\n",
    "\n",
    "price_df = round(raw_data['Close'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a get_price Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "def get_price():\n",
    "   # read first two rows to get headers\n",
    "   head_df = pl.read_csv('data/raw-yfinance.csv', n_rows=2)\n",
    "   # get tickers from first row\n",
    "   cols = ['_'.join([a, str(b)]) \n",
    "           for a, b in zip(head_df.columns, head_df.row(0))]\n",
    "   return (pl.read_csv('data/raw-yfinance.csv', has_header=False, \n",
    "                       skip_rows=3, new_columns=cols)\n",
    "     # convert first column to datetime\n",
    "     .with_columns(pl.col('_None')\n",
    "                   .str.to_datetime('%Y-%m-%d %H:%M:%S%z'))\n",
    "    .rename({'_None': 'Date'})\n",
    "     # columns starting with Close\n",
    "    .select(cs.matches(r'^(Date|Close.*)'))\n",
    "    # rename columns, take value after last _\n",
    "    .pipe(lambda df_: df_.rename(dict(zip(df_.columns, \n",
    "               [c.split('_')[-1] for c in df_.columns]))))\n",
    ")\n",
    "print(get_price())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pl = get_price()\n",
    "cols = price_pl.columns\n",
    "price_pl.equals(pl.DataFrame(price_df\n",
    " .reset_index()\n",
    " .astype({'Date': 'datetime64[ns, UTC]'})\n",
    " .loc[:, cols])\n",
    ")\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.testing as pt\n",
    "pt.assert_frame_equal(price_pl,\n",
    " pl.DataFrame(price_df\n",
    " .reset_index()\n",
    " .astype({'Date': 'datetime64[ns, UTC]'})\n",
    " .loc[:, cols])\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# AssertionError: DataFrames are different (dtypes do not match)\n",
    "# [left]:  {'Date': Datetime(time_unit='us', time_zone='UTC'), 'BRK-B':\n",
    "#      Float64, 'DIS': Float64, 'F': Float64, 'IBM': Float64, 'MMM':\n",
    "#      Float64,...\n",
    "# [right]: {'Date': Datetime(time_unit='ns', time_zone='UTC'), 'BRK-B':\n",
    "#      Float64, 'DIS': Float64, 'F': Float64, 'IBM': Float64, 'MMM':\n",
    "#      Float64,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "def get_price():\n",
    "   # read first two rows to get headers\n",
    "   head_df = pl.read_csv('data/raw-yfinance.csv', n_rows=2)\n",
    "   # get tickers from first row\n",
    "   cols = ['_'.join([a, str(b)]) \n",
    "           for a, b in zip(head_df.columns, head_df.row(0))]\n",
    "   return (pl.read_csv('data/raw-yfinance.csv', has_header=False, \n",
    "                       skip_rows=3, new_columns=cols)\n",
    "     # convert first column to datetime\n",
    "     .with_columns(pl.col('_None').str.to_datetime('%Y-%m-%d %H:%M:%S%z'))\n",
    "    .rename({'_None': 'Date'})\n",
    "     # columns starting with Close\n",
    "    .select(cs.matches(r'^(Date|Close.*)'))\n",
    "    # rename columns, take value after last _\n",
    "    .pipe(lambda df_: df_.rename(dict(zip(df_.columns, \n",
    "                       [c.split('_')[-1] for c in df_.columns]))))\n",
    "    .with_columns(pl.col('Date')\n",
    "                    .cast(pl.Datetime(time_unit='ns', time_zone='UTC')))\n",
    ")\n",
    "tick_mine = get_price()\n",
    "tick_from_pd = pl.DataFrame(price_df.reset_index().astype(\n",
    "   {'Date': 'datetime64[ns, UTC]'})\n",
    " .loc[:, tick_mine.columns])\n",
    "pt.assert_frame_equal(tick_mine, tick_from_pd)\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# AssertionError: DataFrames are different (value mismatch for column\n",
    "#      'DIS')\n",
    "# [left]:  [86.66999816894531, 88.01000213623047, 86.37000274658203,\n",
    "#      84.16999816894531, 87.18000030517578, 86.87999725341797,\n",
    "#      88.9700012207031...\n",
    "# [right]: [86.67, 88.01, 86.37, 84.17, 87.18, 86.88, 88.97, 91.98, 91.92,\n",
    "#      93.92, 94.77, 95.56, 96.33, 99.81, 99.4, 99.91, 99.04, 99.08, 103.4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "def get_price():\n",
    "   # read first two rows to get headers\n",
    "   head_df = pl.read_csv('data/raw-yfinance.csv', n_rows=2)\n",
    "   # get tickers from first row\n",
    "   cols = ['_'.join([a, str(b)]) \n",
    "           for a, b in zip(head_df.columns, head_df.row(0))]\n",
    "   return (pl.read_csv('data/raw-yfinance.csv', has_header=False, \n",
    "                       skip_rows=3, new_columns=cols)\n",
    "    # convert first column to datetime\n",
    "    .with_columns(pl.col('_None').str.to_datetime('%Y-%m-%d %H:%M:%S%z'))\n",
    "    .rename({'_None': 'Date'})\n",
    "    # columns starting with Close\n",
    "    .select(cs.matches(r'^(Date|Close.*)'))\n",
    "    # rename columns, take value after last _\n",
    "    .pipe(lambda df_: df_.rename(dict(zip(df_.columns, \n",
    "                       [c.split('_')[-1] for c in df_.columns]))))\n",
    "    .with_columns(pl.col('Date')\n",
    "                    .cast(pl.Datetime(time_unit='ns', time_zone='UTC')))\n",
    "    .with_columns(cs.float().round(2))\n",
    ")\n",
    "tick_mine = get_price()\n",
    "tick_from_pd = pl.DataFrame(price_df.reset_index().astype(\n",
    "   {'Date': 'datetime64[ns, UTC]'})\n",
    " .loc[:, tick_mine.columns])\n",
    "pt.assert_frame_equal(tick_mine, tick_from_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a get_port Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                'UPS','F']\n",
    "df_data= { \n",
    "'Beta':[1.34,2,0.75,1.2,0.41,0.95,1.23,0.9,1.05,1.15],\n",
    "'Shares':[-1900,-100,-400,-800,-5500,1600,1800,2800,1100,20800],\n",
    "'rSL':[42.75,231,156,54.2,37.5,42.75,29.97,59.97,39.97,2.10]\n",
    "}\n",
    "port = pd.DataFrame(df_data,index=port_tickers)\n",
    "port['Side'] = np.sign(port['Shares'])\n",
    "port['rCost'] = round(price_df.iloc[0,:].div(bm_cost) *1000,2)\n",
    "port['rPrice'] = round(price_df.iloc[-1,:].div(bm_price) *1000,2)\n",
    "port['Cost'] = price_df.iloc[0,:]\n",
    "port['Price'] = price_df.iloc[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_port(price_df):\n",
    "    df_data= { \n",
    "        'Beta':[1.34,2,0.75,1.2,0.41,0.95,1.23,0.9,1.05,1.15],\n",
    "        'Shares':[-1900,-100,-400,-800,-5500,1600,1800,2800,1100,20800],\n",
    "        'rSL':[42.75,231,156,54.2,37.5,42.75,29.97,59.97,39.97,2.10]\n",
    "    }   \n",
    "    port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                   'UPS','F']\n",
    "    cost = (price_df\n",
    "        .select(port_tickers)\n",
    "        .head(1)\n",
    "        .transpose()\n",
    "        .rename({'column_0': 'Cost'})\n",
    "    )\n",
    "    price = (price_df\n",
    "        .select(port_tickers)\n",
    "        .tail(1)\n",
    "        .transpose()\n",
    "        .rename({'column_0': 'Price'})\n",
    "    )\n",
    "    return (pl.DataFrame(df_data)\n",
    "        .with_columns(\n",
    "            tickers=pl.lit(pl.Series(port_tickers)),\n",
    "            Side=pl.when(pl.col('Shares') > 0).then(1).otherwise(-1)\n",
    "                    .cast(pl.Int64)\n",
    "        )\n",
    "        .hstack(cost)\n",
    "        .hstack(price)\n",
    "        .with_columns(rCost=(pl.col('Cost') / bm_cost * 1000).round(2),\n",
    "                      rPrice=(pl.col('Price') / bm_price * 1000).round(2)\n",
    "        )\n",
    "        .select(['tickers', 'Beta', 'Shares', 'rSL', 'Side', 'rCost', \n",
    "                 'rPrice', 'Cost', 'Price'])\n",
    "    )\n",
    "\n",
    "port_pl = get_port(get_price())\n",
    "print(port_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.assert_frame_equal(port_pl, \n",
    "    pl.DataFrame(port.reset_index()).rename({'index': 'tickers'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df['bm returns'] = round(np.exp(np.log(price_df[bm_ticker]/\n",
    "                        price_df[bm_ticker].shift()).cumsum()) - 1, 3)\n",
    "rel_price = round(price_df.div(price_df['^GSPC'],axis=0 )*1000,2)\n",
    "\n",
    "rMV = rel_price.mul(port['Shares'])\n",
    "rLong_MV = rMV[rMV >0].sum(axis=1)\n",
    "rShort_MV = rMV[rMV <0].sum(axis=1)\n",
    "rMV_Beta = rMV.mul(port['Beta'])\n",
    "rLong_MV_Beta = rMV_Beta[rMV_Beta >0].sum(axis=1) / rLong_MV\n",
    "rShort_MV_Beta = rMV_Beta[rMV_Beta <0].sum(axis=1)/ rShort_MV\n",
    "\n",
    "price_df['rNet_Beta'] = rLong_MV_Beta - rShort_MV_Beta\n",
    "price_df['rNet'] = round((rLong_MV + rShort_MV)\n",
    "                         .div(abs(rMV).sum(axis=1)),3)\n",
    "\n",
    "price_df['rReturns_Long'] = round(\n",
    "    np.exp(np.log(rLong_MV/rLong_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns_Short'] = - round(\n",
    "    np.exp(np.log(rShort_MV/rShort_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns'] = price_df['rReturns_Long'] + \\\n",
    "    price_df['rReturns_Short']\n",
    "\n",
    "MV = price_df.mul(port['Shares'])\n",
    "Long_MV = MV[MV >0].sum(axis=1)\n",
    "Short_MV = MV[MV <0].sum(axis=1)\n",
    "price_df['Gross'] = round((Long_MV - Short_MV).div(K),3)\n",
    "price_df['Net'] = round((Long_MV + Short_MV).div(abs(MV).sum(axis=1)),3)\n",
    "\n",
    "price_df['Returns_Long'] = round(\n",
    "    np.exp(np.log(Long_MV/Long_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns_Short'] = -round(\n",
    "    np.exp(np.log(Short_MV/Short_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns'] = price_df['Returns_Long'] + price_df['Returns_Short']\n",
    "\n",
    "MV_Beta = MV.mul(port['Beta'])\n",
    "Long_MV_Beta = MV_Beta[MV_Beta >0].sum(axis=1) / Long_MV\n",
    "Short_MV_Beta = MV_Beta[MV_Beta <0].sum(axis=1)/ Short_MV\n",
    "price_df['Net_Beta'] = Long_MV_Beta - Short_MV_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round(np.exp(np.log(price_df[bm_ticker]/\n",
    "#    price_df[bm_ticker].shift()).cumsum()) - 1, 3)\n",
    "def update_price_df(price_df, bm_ticker):\n",
    "    return (price_df\n",
    "     .with_columns(bm_shift=pl.col(bm_ticker).shift(),\n",
    "      **{'bm returns': (pl.col(bm_ticker)/\n",
    "        (pl.col(bm_ticker).shift())).log().cum_sum().exp().sub(1)})\n",
    "    )\n",
    "\n",
    "pt.assert_frame_equal(update_price_df(get_price(), bm_ticker)\n",
    " .select(['bm returns']),\n",
    " pl.DataFrame(price_df[['bm returns']])\n",
    ")\n",
    "# Traceback (most recent call last)\n",
    "#   ...\n",
    "# AssertionError: DataFrames are different (value mismatch for column 'bm\n",
    "#      returns')\n",
    "# [left]:  [None, 0.005868056373107056, 0.001794688663375732,\n",
    "#      -0.010247515298020149, 0.007034865620724418, 0.004476257001509776,\n",
    "#      0.00045782874...\n",
    "# [right]: [None, 0.006, 0.002, -0.01, 0.007, 0.004, 0.0, 0.008, -0.004,\n",
    "#      0.019, 0.018, 0.025, 0.039, 0.042, 0.046, 0.044, 0.028, 0.02, 0.039, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function and address rounding\n",
    "def returns(col_name, result_name=None):\n",
    "    if result_name is None:\n",
    "        result_name = f'{col_name} returns'\n",
    "    return ((pl.col(col_name)\n",
    "             / pl.col(col_name).shift())\n",
    "            .log()\n",
    "            .cum_sum()\n",
    "            .exp()\n",
    "            .sub(1)\n",
    "            .round(3)\n",
    "            .alias(result_name)\n",
    "    )\n",
    "\n",
    "def update_price_df(price_df, bm_ticker):\n",
    "    return (price_df\n",
    "            .with_columns(returns(bm_ticker, 'bm returns'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.assert_frame_equal(\n",
    "  (update_price_df(get_price(), bm_ticker).select(['bm returns'])),\n",
    "  pl.DataFrame(price_df[['bm returns']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rel_price = round(price_df.div(price_df['^GSPC'],axis=0 )*1000,2)\n",
    "rel_price_pl = (get_price()\n",
    " .with_columns((pl.col(ticker_list) / (pl.col(bm_ticker)) * 1000).round(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rMV = rel_price.mul(port['Shares'])\n",
    "rMV_pl = (rel_price_pl\n",
    " .with_columns([pl.col(name)*shares for i, (name, shares) in \n",
    "                enumerate(port_pl.select(['tickers', 'Shares']).iter_rows())])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rLong_MV = rMV[rMV >0].sum(axis=1)\n",
    "rLong_MV_pl = (rMV_pl\n",
    ".select(pl.when(pl.col(pl.Float64).gt(0))\n",
    "        .then(pl.col(pl.Float64)).otherwise(0))\n",
    ".select(pl.sum_horizontal(pl.all()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rLong_MV = rMV[rMV >0].sum(axis=1)\n",
    "#rMV_Beta = rMV.mul(port['Beta'])\n",
    "#rLong_MV_Beta = rMV_Beta[rMV_Beta >0].sum(axis=1) / rLong_MV\n",
    "rLong_MV_pl = (rMV_pl\n",
    "#.select(p.when(pl.col(pl.Float64).gt(0))\n",
    "#              .then(pl.col(pl.Float64)).otherwise(0))\n",
    ".select(pl.col(pl.Float64).clip(lower_bound=0))\n",
    ".select(pl.sum_horizontal(pl.all()).alias('rLong_MV'))\n",
    ")\n",
    "\n",
    "rMV_Beta_pl = (rMV_pl\n",
    " .with_columns([pl.col(name)*shares for i, (name, shares) in \n",
    "                enumerate(port_pl.select(['tickers', 'Beta']).iter_rows())])\n",
    ")\n",
    "rLong_MV_Beta_pl = (rMV_Beta_pl\n",
    " .with_columns(pl.col(pl.Float64).clip(lower_bound=0))\n",
    " .select('Date', \n",
    "        rLong_MV_Beta=(pl.sum_horizontal(pl.col(pl.Float64)) / \n",
    "                       rLong_MV_pl['rLong_MV'])\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rShort_MV = rMV[rMV <0].sum(axis=1)\n",
    "#rShort_MV_Beta = rMV_Beta[rMV_Beta <0].sum(axis=1)/ rShort_MV\n",
    "rShort_MV_pl = (rMV_pl\n",
    "                .select(pl.col(pl.Float64).clip(upper_bound=0))\n",
    ".select(pl.sum_horizontal(pl.all()).alias('rShort_MV'))\n",
    ")\n",
    "\n",
    "rShort_MV_Beta_pl = (rMV_Beta_pl\n",
    ".with_columns(pl.col(pl.Float64).clip(upper_bound=0))\n",
    ".select('Date', \n",
    "        rShort_MV_Beta=(pl.sum_horizontal(pl.col(pl.Float64)) / \n",
    "                        rShort_MV_pl['rShort_MV']))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need .to_series() \n",
    "print((update_price_df(get_price(), bm_ticker))\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "          - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## added atol=1e-4 to get rid of rounding error\n",
    "pt.assert_frame_equal((update_price_df(get_price(), bm_ticker)\n",
    "    .with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "            - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series())\n",
    "     .select(['rNet_Beta'])),\n",
    "    pl.DataFrame(price_df[['rNet_Beta']]), atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df['rReturns_Long'] = round(\n",
    "#     np.exp(np.log(rLong_MV/rLong_MV.shift()).cumsum())-1,3)\n",
    "# price_df['rReturns_Short'] = - round(\n",
    "#     np.exp(np.log(rShort_MV/rShort_MV.shift()).cumsum())-1,3)\n",
    "# price_df['rReturns'] = price_df['rReturns_Long']  \\\n",
    "#          + price_df['rReturns_Short']\n",
    "\n",
    "(update_price_df(get_price(), bm_ticker)\n",
    " .with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "                - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV']\n",
    "              )\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.assert_frame_equal(update_price_df(get_price(), bm_ticker)\n",
    " .with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "         - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV']\n",
    "              )\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'))\n",
    " .select(['rReturns']), \n",
    "pl.DataFrame(price_df[['rReturns']]), atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MV = price_df.mul(port['Shares'])\n",
    "# Long_MV = MV[MV >0].sum(axis=1)\n",
    "# Short_MV = MV[MV <0].sum(axis=1)\n",
    "# price_df['Gross'] = round((Long_MV - Short_MV).div(K),3)\n",
    "# price_df['Net'] = round(\n",
    "#    (Long_MV + Short_MV).div(abs(MV).sum(axis=1)),3)\n",
    "\n",
    "MV_pl = (price_pl\n",
    "    .with_columns([pl.col(ticker)*port_pl.select('Shares').row(i)[0] \n",
    "                   for i, ticker in enumerate(port_pl['tickers'])])\n",
    "    .with_columns(pl.lit(0).alias(bm_ticker))   # clear ^GSPC column\n",
    ")\n",
    "Long_MV_pl = (MV_pl\n",
    ".select(pl.col(pl.Float64).clip(lower_bound=0))\n",
    ".select(Long_MV=pl.sum_horizontal(pl.all()))\n",
    ")\n",
    "Short_MV_pl = (MV_pl\n",
    ".select(pl.col(pl.Float64).clip(upper_bound=0))\n",
    ".select(Short_MV=pl.sum_horizontal(pl.all()))\n",
    ")\n",
    "\n",
    "net_denom = (MV_pl\n",
    ".select(pl.col(pl.Float64))\n",
    ".select(pl.sum_horizontal(pl.all().abs()))\n",
    ")\n",
    "\n",
    "(update_price_df(get_price(), bm_ticker)\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "            - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV'],\n",
    "    Long_MV=Long_MV_pl['Long_MV'],\n",
    "    Short_MV=Short_MV_pl['Short_MV'])\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "               Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "               Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "               net_denom.to_series())\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.assert_frame_equal((update_price_df(get_price(), bm_ticker)\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "                  - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV'],\n",
    "    Long_MV=Long_MV_pl['Long_MV'],\n",
    "    Short_MV=Short_MV_pl['Short_MV'])\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "               Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "                Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "                 net_denom.to_series())\n",
    " )\n",
    " .select(['Gross', 'Net'])),\n",
    "    pl.DataFrame(price_df[['Gross', 'Net']]), atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df['Returns_Long'] = round(\n",
    "#     np.exp(np.log(Long_MV/Long_MV.shift()).cumsum())-1,3)\n",
    "# price_df['Returns_Short'] = - round(\n",
    "#     np.exp(np.log(Short_MV/Short_MV.shift()).cumsum())-1,3)\n",
    "# price_df['Returns'] = price_df['Returns_Long'] + price_df['Returns_Short']\n",
    "\n",
    "\n",
    "(update_price_df(get_price(), bm_ticker)\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "               - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV'],\n",
    "    Long_MV=Long_MV_pl['Long_MV'],\n",
    "    Short_MV=Short_MV_pl['Short_MV'])\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "               Returns_Long=returns('Long_MV'),\n",
    "               Returns_Short=-returns('Short_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "               Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "               Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "                 net_denom.to_series()),\n",
    "               Returns=((pl.col('Returns_Long') + pl.col('Returns_Short')))              \n",
    " )\n",
    " .select(['Returns', 'Returns_Long', 'Returns_Short'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.assert_frame_equal((update_price_df(get_price(), bm_ticker)\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "               - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV'],\n",
    "    Long_MV=Long_MV_pl['Long_MV'],\n",
    "    Short_MV=Short_MV_pl['Short_MV'])\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "               Returns_Long=returns('Long_MV'),\n",
    "               Returns_Short=-returns('Short_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "               Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "                Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "                 net_denom.to_series()),\n",
    "                Returns=((pl.col('Returns_Long') + pl.col('Returns_Short')))              \n",
    " )\n",
    " .select(['Returns'])\n",
    "),\n",
    "pl.DataFrame(price_df[['Returns']]), atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MV_Beta = MV.mul(port['Beta'])\n",
    "# Long_MV_Beta = MV_Beta[MV_Beta >0].sum(axis=1) / Long_MV\n",
    "# Short_MV_Beta = MV_Beta[MV_Beta <0].sum(axis=1)/ Short_MV\n",
    "# price_df['Net_Beta'] = Long_MV_Beta - Short_MV_Beta\n",
    "\n",
    "MV_Beta_pl = (MV_pl\n",
    "    .with_columns([pl.col(ticker)*port_pl.select('Beta').row(i)[0] \n",
    "                   for i, ticker in enumerate(port_pl['tickers'])])\n",
    ")\n",
    "Long_MV_Beta_pl = (MV_Beta_pl\n",
    "                   # use clip\n",
    " .select(pl.col(pl.Float64).clip(lower_bound=0))\n",
    " .select(Long_MV_Beta=pl.sum_horizontal(pl.all()) / \n",
    "         Long_MV_pl['Long_MV'])\n",
    ")\n",
    "\n",
    "Short_MV_Beta_pl = (MV_Beta_pl\n",
    " .select(pl.col(pl.Float64).clip(upper_bound=0))\n",
    " .select(Short_MV_Beta=pl.sum_horizontal(pl.all()) / \n",
    "         Short_MV_pl['Short_MV'] )\n",
    ")\n",
    "\n",
    "(update_price_df(get_price(), bm_ticker)\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('rLong_MV_Beta') \n",
    "            - rShort_MV_Beta_pl.select('rShort_MV_Beta')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['rLong_MV'], \n",
    "    rShort_MV=rShort_MV_pl['rShort_MV'],\n",
    "    Long_MV=Long_MV_pl['Long_MV'],\n",
    "    Short_MV=Short_MV_pl['Short_MV'])\n",
    " .with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "               rReturns_Short=-returns('rShort_MV'),\n",
    "               Returns_Long=returns('Long_MV'),\n",
    "               Returns_Short=-returns('Short_MV'),\n",
    "                )\n",
    " .with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "    Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "    Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "        net_denom.to_series()),\n",
    "    Returns=((pl.col('Returns_Long') + pl.col('Returns_Short'))),\n",
    "    Net_Beta=(Long_MV_Beta_pl['Long_MV_Beta'] - \n",
    "              Short_MV_Beta_pl['Short_MV_Beta'])           \n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "def returns(col_name, result_name=None):\n",
    "    if result_name is None:\n",
    "        result_name = f'{col_name} returns'\n",
    "    return ((pl.col(col_name)\n",
    "             / pl.col(col_name).shift())\n",
    "            .log()\n",
    "            .cum_sum()\n",
    "            .exp()\n",
    "            .sub(1)\n",
    "            .round(3)\n",
    "            .alias(result_name)\n",
    "    )\n",
    "\n",
    "def calc_long(df_, divisor=1, alias='long'):\n",
    "    return (df_\n",
    "            .select(pl.col(pl.Float64).clip(lower_bound=0))\n",
    "            .select((pl.sum_horizontal(pl.all()) / divisor).alias(alias))\n",
    "              )\n",
    "\n",
    "def calc_short(df_, divisor=1, alias='short'):\n",
    "    return (df_\n",
    "            .select(pl.col(pl.Float64).clip(upper_bound=0))\n",
    "            .select((pl.sum_horizontal(pl.all()) / divisor).alias(alias))\n",
    "              )\n",
    "\n",
    "def multiply_tickers_rows_by_column(df_, col_name, port_pl):\n",
    "    return (df_\n",
    "            .with_columns([pl.col(ticker)*port_pl.select(col_name).row(i)[0] \n",
    "                           for i, ticker in enumerate(port_pl['tickers'])])\n",
    "           )\n",
    "\n",
    "\n",
    "def get_price():\n",
    "   # read first two rows to get headers\n",
    "   head_df = pl.read_csv('data/raw-yfinance.csv', n_rows=2)\n",
    "   # get tickers from first row\n",
    "   cols = ['_'.join([a, str(b)]) for a, b in \n",
    "           zip(head_df.columns, head_df.row(0))]\n",
    "   return (pl.read_csv('data/raw-yfinance.csv', has_header=False, \n",
    "                       skip_rows=3, new_columns=cols, try_parse_dates=True)\n",
    "     # convert first column to datetime\n",
    "     #.with_columns(pl.col('_None').str.to_datetime('%Y-%m-%d %H:%M:%S%z'))\n",
    "    .rename({'_None': 'Date'})\n",
    "     # columns starting with Close\n",
    "    .select(cs.matches(r'^(Date|Close.*)'))\n",
    "    # rename columns, take value after last _\n",
    "    .pipe(lambda df_: df_.rename(dict(zip(df_.columns, \n",
    "                            [c.split('_')[-1] for c in df_.columns]))))\n",
    "    .with_columns(\n",
    "        pl.col('Date').cast(pl.Datetime(time_unit='ns', time_zone='UTC')),\n",
    "        pl.col('NFLX').cast(pl.Float64))\n",
    "    .with_columns(cs.float().round(2))\n",
    ")\n",
    "\n",
    "def get_port(price_df, bm_ticker):\n",
    "    df_data= { \n",
    "        'Beta':[1.34,2,0.75,1.2,0.41,0.95,1.23,0.9,1.05,1.15],\n",
    "        'Shares':[-1900,-100,-400,-800,-5500,1600,1800,2800,1100,20800],\n",
    "        'rSL':[42.75,231,156,54.2,37.5,42.75,29.97,59.97,39.97,2.10]\n",
    "    }   \n",
    "    port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                  'UPS','F']\n",
    "    cost = (price_df\n",
    "        .select(port_tickers)\n",
    "        .head(1)\n",
    "        .transpose()\n",
    "        .rename({'column_0': 'Cost'})\n",
    "    )\n",
    "    price = (price_df\n",
    "        .select(port_tickers)\n",
    "        .tail(1)\n",
    "        .transpose()\n",
    "        .rename({'column_0': 'Price'})\n",
    "    )\n",
    "    bm_cost = price_df[bm_ticker][0]\n",
    "    bm_price = price_df[bm_ticker][-1]\n",
    "    return (pl.DataFrame(df_data)\n",
    "        .with_columns(\n",
    "            tickers=pl.lit(pl.Series(port_tickers)),\n",
    "            Side=pl.when(pl.col('Shares') > 0).then(1).otherwise(-1)\n",
    "                    .cast(pl.Int64)\n",
    "        )\n",
    "        .hstack(cost)\n",
    "        .hstack(price)\n",
    "        .with_columns(rCost=(pl.col('Cost') / bm_cost * 1000).round(2),\n",
    "                      rPrice=(pl.col('Price') / bm_price * 1000).round(2)\n",
    "        )\n",
    "        .select(['tickers', 'Beta', 'Shares', 'rSL', 'Side', 'rCost', \n",
    "                 'rPrice', 'Cost', 'Price'])\n",
    "    )\n",
    "\n",
    "def get_price_and_port():\n",
    "    K = 1000000\n",
    "    lot = 100\n",
    "    port_tickers = ['QCOM','TSLA','NFLX','DIS','PG', 'MMM','IBM','BRK-B',\n",
    "                    'UPS','F']\n",
    "    bm_ticker = '^GSPC'\n",
    "    ticker_list = [bm_ticker] + port_tickers\n",
    "    price_pl = get_price()\n",
    "    port_pl = get_port(price_pl, bm_ticker)\n",
    "\n",
    "    # inline rel_price\n",
    "    rMV_pl = (price_pl\n",
    "            .with_columns((pl.col(ticker_list) / \n",
    "                           (pl.col(bm_ticker)) * 1000).round(2))\n",
    "            .pipe(multiply_tickers_rows_by_column, 'Shares', port_pl)\n",
    "    )\n",
    "    rMV_Beta_pl = (rMV_pl\n",
    "                .pipe(multiply_tickers_rows_by_column, 'Beta', port_pl)\n",
    "    )\n",
    "    rShort_MV_pl = rMV_pl.pipe(calc_short)\n",
    "    rShort_MV_Beta_pl = rMV_Beta_pl.pipe(calc_short, \n",
    "                                         divisor=rShort_MV_pl['short'])\n",
    "\n",
    "    rLong_MV_pl = rMV_pl.pipe(calc_long)\n",
    "    rLong_MV_Beta_pl = rMV_Beta_pl.pipe(calc_long, divisor=rLong_MV_pl['long'])\n",
    "\n",
    "    MV_pl = (price_pl\n",
    "        .pipe(multiply_tickers_rows_by_column, 'Shares', port_pl)\n",
    "        .with_columns(pl.lit(0).alias(bm_ticker))   # clear ^GSPC column\n",
    "    )\n",
    "    Long_MV_pl = MV_pl.pipe(calc_long)\n",
    "    Short_MV_pl = MV_pl.pipe(calc_short)\n",
    "\n",
    "    net_denom = (MV_pl\n",
    "        .select(pl.col(pl.Float64))\n",
    "        .select(pl.sum_horizontal(pl.all().abs()))\n",
    "    )\n",
    "\n",
    "    MV_Beta_pl = (MV_pl\n",
    "        .pipe(multiply_tickers_rows_by_column, 'Beta', port_pl)\n",
    "    )\n",
    "    Long_MV_Beta_pl = MV_Beta_pl.pipe(calc_long, \n",
    "                                      divisor=Long_MV_pl['long'])\n",
    "    Short_MV_Beta_pl = MV_Beta_pl.pipe(calc_short,\n",
    "                                       divisor=Short_MV_pl['short'])\n",
    "\n",
    "    final_price = (price_pl\n",
    "        .with_columns(returns(bm_ticker))\n",
    "        .with_columns(\n",
    "            rNet_Beta=(rLong_MV_Beta_pl.select('long') \n",
    "                    - rShort_MV_Beta_pl.select('short')).to_series(),\n",
    "            rLong_MV=rLong_MV_pl['long'], \n",
    "            rShort_MV=rShort_MV_pl['short'],\n",
    "            Long_MV=Long_MV_pl['long'],\n",
    "            Short_MV=Short_MV_pl['short']\n",
    "            )\n",
    "        .with_columns(\n",
    "            rReturns_Long=returns('rLong_MV'),\n",
    "            rReturns_Short=-returns('rShort_MV'),\n",
    "            Returns_Long=returns('Long_MV'),\n",
    "            Returns_Short=-returns('Short_MV'),\n",
    "            )\n",
    "        .with_columns(\n",
    "            rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "            Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "            Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "                net_denom.to_series()),\n",
    "            Returns=((pl.col('Returns_Long') + pl.col('Returns_Short'))),\n",
    "            Net_Beta=(Long_MV_Beta_pl['long'] - Short_MV_Beta_pl['short'])           \n",
    "            )\n",
    "    )\n",
    "    return final_price, port_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.testing as pt\n",
    "price_pl, port_pl = get_price_and_port()\n",
    "pt.assert_frame_equal(\n",
    "    price_pl.select(['Gross', 'Net', 'Returns', 'Net_Beta']),\n",
    "    pl.DataFrame(price_df[['Gross', 'Net', 'Returns', 'Net_Beta']]), \n",
    "    atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Port or Not to Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "price_df = round( raw_data['Close'],2)\n",
    "\n",
    "bm_cost = price_df[bm_ticker].iloc[0]\n",
    "bm_price = price_df[bm_ticker].iloc[-1]\n",
    "\n",
    "port['rCost'] = round(price_df.iloc[0,:].div(bm_cost) *1000,2)\n",
    "port['rPrice'] = round(price_df.iloc[-1,:].div(bm_price) *1000,2)\n",
    "port['Cost'] = price_df.iloc[0,:]\n",
    "port['Price'] = price_df.iloc[-1,:]\n",
    "\n",
    "# Chapter 13: Portfolio Management System\n",
    "\n",
    "price_df['bm returns'] = round(\n",
    "    np.exp(np.log(price_df[bm_ticker]/\n",
    "    price_df[bm_ticker].shift()).cumsum()) - 1, 3)\n",
    "rel_price = round(price_df.div(price_df['^GSPC'],axis=0 )*1000,2)\n",
    "\n",
    "rMV = rel_price.mul(port['Shares'])\n",
    "rLong_MV = rMV[rMV >0].sum(axis=1)\n",
    "rShort_MV = rMV[rMV <0].sum(axis=1)\n",
    "rMV_Beta = rMV.mul(port['Beta'])\n",
    "rLong_MV_Beta = rMV_Beta[rMV_Beta >0].sum(axis=1) / rLong_MV\n",
    "rShort_MV_Beta = rMV_Beta[rMV_Beta <0].sum(axis=1)/ rShort_MV\n",
    "\n",
    "price_df['rNet_Beta'] = rLong_MV_Beta - rShort_MV_Beta\n",
    "price_df['rNet'] = round(\n",
    "    (rLong_MV + rShort_MV).div(abs(rMV).sum(axis=1)),3)\n",
    "\n",
    "price_df['rReturns_Long'] = round(\n",
    "    np.exp(np.log(rLong_MV/rLong_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns_Short'] = - round(\n",
    "    np.exp(np.log(rShort_MV/rShort_MV.shift()).cumsum())-1,3)\n",
    "price_df['rReturns'] = price_df['rReturns_Long'] + \\\n",
    "    price_df['rReturns_Short']\n",
    "\n",
    "MV = price_df.mul(port['Shares'])\n",
    "Long_MV = MV[MV >0].sum(axis=1)\n",
    "Short_MV = MV[MV <0].sum(axis=1)\n",
    "price_df['Gross'] = round((Long_MV - Short_MV).div(K),3)\n",
    "price_df['Net'] = round(\n",
    "    (Long_MV + Short_MV).div(abs(MV).sum(axis=1)),3)\n",
    "\n",
    "price_df['Returns_Long'] = round(\n",
    "    np.exp(np.log(Long_MV/Long_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns_Short'] = - round(\n",
    "    np.exp(np.log(Short_MV/Short_MV.shift()).cumsum())-1,3)\n",
    "price_df['Returns'] = price_df['Returns_Long'] + \\\n",
    "      price_df['Returns_Short']\n",
    "\n",
    "MV_Beta = MV.mul(port['Beta'])\n",
    "Long_MV_Beta = MV_Beta[MV_Beta >0].sum(axis=1) / Long_MV\n",
    "Short_MV_Beta = MV_Beta[MV_Beta <0].sum(axis=1)/ Short_MV\n",
    "price_df['Net_Beta'] = Long_MV_Beta - Short_MV_Beta\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# inline rel_price\n",
    "rMV_pl = (price_pl\n",
    "    .with_columns((pl.col(ticker_list) / \n",
    "            (pl.col(bm_ticker)) * 1000).round(2))\n",
    "    .pipe(multiply_tickers_rows_by_column, 'Shares', port_pl)\n",
    ")\n",
    "rMV_Beta_pl = (rMV_pl\n",
    "            .pipe(multiply_tickers_rows_by_column, 'Beta', port_pl)\n",
    ")\n",
    "rShort_MV_pl = (rMV_pl.pipe(calc_short))\n",
    "rShort_MV_Beta_pl = (rMV_Beta_pl.pipe(calc_short,\n",
    "                     divisor=rShort_MV_pl['short']))\n",
    "\n",
    "rLong_MV_pl = (rMV_pl.pipe(calc_long))\n",
    "rLong_MV_Beta_pl = (rMV_Beta_pl.pipe(calc_long,\n",
    "                    divisor=rLong_MV_pl['long']))\n",
    "\n",
    "MV_pl = (price_pl\n",
    "    .pipe(multiply_tickers_rows_by_column, 'Shares', port_pl)\n",
    "    .with_columns(pl.lit(0).alias(bm_ticker))   # clear ^GSPC column\n",
    ")\n",
    "Long_MV_pl = (MV_pl.pipe(calc_long))\n",
    "Short_MV_pl = (MV_pl.pipe(calc_short))\n",
    "\n",
    "net_denom = (MV_pl\n",
    ".select(pl.col(pl.Float64))\n",
    ".select(pl.sum_horizontal(pl.all().abs()))\n",
    ")\n",
    "\n",
    "MV_Beta_pl = (MV_pl\n",
    "    .pipe(multiply_tickers_rows_by_column, 'Beta', port_pl)\n",
    ")\n",
    "Long_MV_Beta_pl = (MV_Beta_pl.pipe(calc_long, divisor=Long_MV_pl['long']))\n",
    "Short_MV_Beta_pl = (MV_Beta_pl.pipe(calc_short,\n",
    "                    divisor=Short_MV_pl['short']))\n",
    " \n",
    "final_price = (price_pl\n",
    ".with_columns(returns(bm_ticker))\n",
    ".with_columns(rNet_Beta=(rLong_MV_Beta_pl.select('long') - \n",
    "              rShort_MV_Beta_pl.select('short')).to_series(),\n",
    "    rLong_MV=rLong_MV_pl['long'], \n",
    "    rShort_MV=rShort_MV_pl['short'],\n",
    "    Long_MV=Long_MV_pl['long'],\n",
    "    Short_MV=Short_MV_pl['short']\n",
    "    )\n",
    ".with_columns(rReturns_Long=returns('rLong_MV'),\n",
    "            rReturns_Short=-returns('rShort_MV'),\n",
    "            Returns_Long=returns('Long_MV'),\n",
    "            Returns_Short=-returns('Short_MV'),\n",
    "    )\n",
    ".with_columns(rReturns=pl.col('rReturns_Long') + pl.col('rReturns_Short'),\n",
    "            Gross=((pl.col('Long_MV') - pl.col('Short_MV')) / K),\n",
    "            Net=((pl.col('Long_MV') + pl.col('Short_MV')) / \n",
    "                net_denom.to_series()),\n",
    "            Returns=((pl.col('Returns_Long') + pl.col('Returns_Short'))),\n",
    "            Net_Beta=(Long_MV_Beta_pl['long'] - Short_MV_Beta_pl['short'])           \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Methods\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Extending Polars\n",
    "\n",
    "## Loans with Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 600_000\n",
    "c = 0.055 / 12\n",
    "n = 360\n",
    "P = (L * \n",
    "   (c * (1 + c) ** n) / \\\n",
    "   ((1 + c) ** n - 1))\n",
    "P  # monthly payment 3_406.734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payment_schedule_gen(principal, number_of_payments, \n",
    "                         monthly_interest_rate, monthly_payment):\n",
    "    remaining_balance = principal\n",
    "    done = False\n",
    "    for month in range(number_of_payments):\n",
    "        if remaining_balance < monthly_payment:\n",
    "            interest_payment = remaining_balance * monthly_interest_rate\n",
    "            monthly_payment = remaining_balance + interest_payment\n",
    "            principal_payment = remaining_balance\n",
    "            remaining_balance = 0\n",
    "            done = True\n",
    "        else:\n",
    "            interest_payment = remaining_balance * monthly_interest_rate\n",
    "            principal_payment = monthly_payment - interest_payment\n",
    "            remaining_balance -= principal_payment\n",
    "        yield {'month': month,\n",
    "                'Principal': principal_payment,\n",
    "                'Interest': interest_payment,\n",
    "                'Remaining Balance': remaining_balance,\n",
    "                'Monthly Payment': monthly_payment}\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame(payment_schedule_gen(L, n, c, P)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pl.DataFrame(payment_schedule_gen(L, n, c, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def payment_schedule_numba(principal, number_of_payments, \n",
    "                           monthly_interest_rate, monthly_payment):\n",
    "    remaining_balance = principal\n",
    "    done = False\n",
    "    results = np.zeros((number_of_payments, 5), dtype=np.float64)\n",
    "    for month in range(number_of_payments):\n",
    "        if remaining_balance < monthly_payment:\n",
    "            interest_payment = remaining_balance * monthly_interest_rate\n",
    "            monthly_payment = remaining_balance + interest_payment\n",
    "            principal_payment = remaining_balance\n",
    "            remaining_balance = 0\n",
    "            done = True\n",
    "        else:\n",
    "            interest_payment = remaining_balance * monthly_interest_rate\n",
    "            principal_payment = monthly_payment - interest_payment\n",
    "            remaining_balance -= principal_payment\n",
    "        results[month, 0] = month\n",
    "        results[month, 1] = principal_payment\n",
    "        results[month, 2] = interest_payment\n",
    "        results[month, 3] = remaining_balance\n",
    "        results[month, 4] = monthly_payment\n",
    "        if done:\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.DataFrame(payment_schedule_numba(L, n, c, P))\n",
    "  .rename({'column_0': 'month', 'column_1': 'Principal', \n",
    "           'column_2': 'Interest', 'column_3': 'Remaining Balance', \n",
    "           'column_4': 'Monthly Payment'}))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(pl.DataFrame(payment_schedule_numba(L, n, c, P))\n",
    "  .rename({'column_0': 'month', 'column_1': 'Principal', \n",
    "           'column_2': 'Interest', 'column_3': 'Remaining Balance', \n",
    "           'column_4': 'Monthly Payment'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = pl.DataFrame({'month': np.arange(360)})\n",
    "\n",
    "def remaining_balance_pl(principal, number_of_payments, \n",
    "                         monthly_interest_rate, num_month):\n",
    "    return principal * ((\n",
    "          (1 + monthly_interest_rate)**number_of_payments - \\\n",
    "          ( 1 + monthly_interest_rate)**num_month ) / \n",
    "       ((1 + monthly_interest_rate)**number_of_payments - 1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sched\n",
    " .with_columns(\n",
    "     remaining_balance_pl(L, n, c, \n",
    "          pl.col('month')).alias('Remaining Balance'),\n",
    "     pl.lit(P).alias('Monthly Payment'))\n",
    " .with_columns(Interest=(pl.col('Remaining Balance') * c))\n",
    " .with_columns(Principal=(pl.col('Monthly Payment') - pl.col('Interest')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(sched\n",
    " .with_columns(\n",
    "     remaining_balance_pl(L, n, c, \n",
    "          pl.col('month')).alias('Remaining Balance'),\n",
    "     pl.lit(P).alias('Monthly Payment'))\n",
    " .with_columns(Interest=(pl.col('Remaining Balance') * c))\n",
    " .with_columns(Principal=\n",
    "       (pl.col('Monthly Payment') - pl.col('Interest')))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a PCA API to Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos = tweak_auto(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars.selectors as cs\n",
    "X = (autos\n",
    "     .select(cs.numeric().fill_null(0))\n",
    "     .select((pl.all() - pl.all().mean()) / pl.all().std()) # 1\n",
    ")\n",
    "num_df = X\n",
    "centered = (num_df # 2\n",
    "            .select((pl.all() - pl.all().mean()))\n",
    "           )\n",
    "cov = np.cov(centered.transpose()) # 3\n",
    "vals, vecs = np.linalg.eig(cov) # 4\n",
    "\n",
    "exp_var = pl.DataFrame( # 5\n",
    "    {'PC': [f'PC{i+1}' for i in range(len(num_df.columns))],\n",
    "     'var':sorted(vals, reverse=True)})\n",
    "\n",
    "idxs = np.argsort(vals)[::-1]\n",
    "comps = (pl.DataFrame(vecs[:, idxs]) # 6\n",
    "         .rename(mapping={f'column_{i}': f'PC{i+1}' \n",
    "                          for i in range(len(num_df.columns))})\n",
    ")\n",
    "\n",
    "pcas = (pl.DataFrame(np.dot(centered, comps)) # 7\n",
    "    .rename(mapping={f'column_{i}': f'PC{i+1}' \n",
    "                     for i in range(len(num_df.columns))})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='polars')\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot of the Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "def plot_pca_3d(df, x='PC1', y='PC2', color_col=None, size_col=None, \n",
    "                symbol_col=None, cmap='viridis', components=None, \n",
    "                biplot=True, biplot_scale=20, biplot_limit=.2, \n",
    "                alpha=1, width=600, height=600):\n",
    "        \n",
    "        if color_col is not None:\n",
    "            data = (df\n",
    "                    .with_columns(color_col)\n",
    "                   )\n",
    "        else:\n",
    "            data = df\n",
    "\n",
    "        fig = px.scatter_3d(data, x='PC1', y='PC2', z='PC3', color=color_col,\n",
    "                            color_continuous_scale=cmap,\n",
    "                            hover_data=df.columns[:10], \n",
    "                            symbol=symbol_col,\n",
    "                            size=size_col,\n",
    "                            opacity=alpha,\n",
    "                            width=width, height=height,\n",
    "                       #log_y=True,\n",
    "                      )\n",
    "\n",
    "        if size_col is None:\n",
    "            fig.update_traces(marker_size=3)\n",
    "\n",
    "        if biplot:\n",
    "            scale = biplot_scale\n",
    "            annots = []\n",
    "            for column in components.columns:\n",
    "                loadings = {f'PC{i}':val*scale \n",
    "                            for i, val in enumerate(components[column], 1)}\n",
    "                loadings['name'] = [column]\n",
    "                new_fig = px.line_3d(x=[0, loadings['PC1']], \n",
    "                                     y=[0, loadings['PC2']],\n",
    "                                     z=[0, loadings['PC3']], width=20)\n",
    "                for trace in new_fig['data']:\n",
    "                    fig.append_trace(trace, row=1, col=1)\n",
    "\n",
    "                annots.append(\n",
    "                    dict(\n",
    "                        showarrow=False,\n",
    "                        x=loadings['PC1'],\n",
    "                        y=loadings['PC2'],\n",
    "                        z=loadings['PC3'],\n",
    "                        text=column,\n",
    "                        xanchor=\"left\",\n",
    "                        xshift=1,\n",
    "                        opacity=0.7)\n",
    "                )\n",
    "            fig.update_layout(scene={'annotations':annots})\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_3d(pcas, color_col=autos['year'], biplot=False, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "@pl.api.register_dataframe_namespace('pca')\n",
    "class PCA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def fit(self):\n",
    "        centered = (self.df\n",
    "                    .select(pl.all() - pl.all().mean())\n",
    "                   )\n",
    "        cov = np.cov(centered.transpose())                     \n",
    "        vals, vecs = np.linalg.eig(cov)\n",
    "        self._explained_variance = pl.DataFrame(\n",
    "            {'PC': [f'PC{i+1}' for i in range(len(num_df.columns))],\n",
    "             'var':sorted(vals, reverse=True)}\n",
    "        )\n",
    "\n",
    "        idxs = np.argsort(vals)[::-1]\n",
    "        comps = (pl.DataFrame(vecs[:, idxs])\n",
    "            .rename(mapping={f'column_{i}': f'PC{i+1}' \n",
    "                for i in range(len(num_df.columns))})\n",
    "        )\n",
    "\n",
    "        self.pcs = (pl.DataFrame(np.dot(centered, comps))\n",
    "            .rename(mapping={f'column_{i}': f'PC{i+1}' \n",
    "                for i in range(len(num_df.columns))})\n",
    "        )    \n",
    "        self._components = comps\n",
    "        return self.df\n",
    "                \n",
    "    def transform(self):\n",
    "        return self.pcs\n",
    "\n",
    "    def explained_variance(self):\n",
    "        return self._explained_variance\n",
    "\n",
    "    def components(self):\n",
    "        return (self._components\n",
    "                .with_columns(Feature=pl.Series(self.df.columns))\n",
    "               )\n",
    "\n",
    "    def filter_components(self, limit_components, mag_threshold):\n",
    "        comps = self.components()\n",
    "        columns = comps.columns[:limit_components]\n",
    "        res =  (comps\n",
    "           .select(*columns, pl.col('Feature'))\n",
    "           .filter(pl.any_horizontal(cs.numeric().abs() > mag_threshold))        \n",
    "        )\n",
    "        return res\n",
    "    \n",
    "    def component_plot(self, limit_components=3, mag_threshold=.1):\n",
    "        comps = self.filter_components(limit_components, mag_threshold)\n",
    "        return  (comps\n",
    "                 .select(cs.numeric())\n",
    "                 .transpose()\n",
    "                 .rename(mapping={f'column_{i}': col\n",
    "                       for i, col in enumerate(comps['Feature'])})\n",
    "                 .select(pl.Series([f'PC{i+1}' \n",
    "                          for i in range(limit_components)]).alias('PC'),\n",
    "                        *comps['Feature'])\n",
    "            .plot.bar(x='PC',\n",
    "                  rot=90)\n",
    "        )\n",
    "    \n",
    "    def scatter3d_plot(self, x='PC1', y='PC2', z='PC3', color_col=None, \n",
    "                       size_col=None, symbol_col=None, cmap='viridis', \n",
    "                       biplot=True, biplot_scale=20, biplot_limit=.2, \n",
    "                       alpha=1, width=600, height=600):\n",
    "        return plot_pca_3d(self.pcs, x, y, color_col, size_col, \n",
    "                symbol_col, cmap, self._components, biplot, biplot_scale, \n",
    "                biplot_limit, alpha, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = (X\n",
    " .pca.fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.pca.explained_variance()\n",
    " .select(pl.all() / pl.all().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.pca.component_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "## Exercises\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## One Last Thing"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".qmd",
    "format_name": "quarto",
    "format_version": "1.0",
    "jupytext_version": "1.15.1"
   }
  },
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
